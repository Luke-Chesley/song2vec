{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from utils import *\n",
    "from feature_encoder import ConvFeatureExtractionModel\n",
    "from gumbel import GumbelVectorQuantizer\n",
    "import math\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 220500])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = []\n",
    "for i in range(1,3):\n",
    "    path = f\"data/mp3_train_files/Gould/Gould - WTC_clip_{i}.mp3\"\n",
    "    waveform,sample_rate = torchaudio.load(path)\n",
    "    waveform = torch.mean(waveform, dim=0).unsqueeze(0)\n",
    "    waveform = normalize_tensor(waveform)\n",
    "    x.append(waveform)\n",
    "\n",
    "\n",
    "x = torch.cat(x)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_waveform(x, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "receptive_field=400 samples\n",
      "9.07 ms\n"
     ]
    }
   ],
   "source": [
    "dim = 512 # quadratic to number of params in conv... double dim quadruple params\n",
    "\n",
    "conv_feature_layers = (\n",
    "    [(dim, 10, 2)] * 3 + [(dim, 3, 2)] * 5 + [(dim, 2, 2)] + [(dim, 2, 2)]\n",
    ")  # (dim,kernel width, strides) # kernel_width is window, string is len of slide\n",
    "conv_feature_layers = [(dim, 10, 5)] + [(dim, 3, 2)] * 4 + [(dim, 2, 2)] + [(dim, 2, 2)]\n",
    "receptive_field = calculate_receptive_field(conv_feature_layers)\n",
    "print(f\"{receptive_field=} samples\")\n",
    "print(f\"{round(receptive_field / (sample_rate / 1000),3)} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num params:  4200448\n",
      "input shape= torch.Size([2, 220500])\n",
      "torch.Size([2, 1, 220500])\n",
      "torch.Size([2, 512, 44099])\n",
      "torch.Size([2, 512, 22049])\n",
      "torch.Size([2, 512, 11024])\n",
      "torch.Size([2, 512, 5511])\n",
      "torch.Size([2, 512, 2755])\n",
      "torch.Size([2, 512, 1377])\n",
      "torch.Size([2, 512, 688])\n",
      "output shape=  torch.Size([2, 512, 688])\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = ConvFeatureExtractionModel(conv_feature_layers, mode=\"default\",dropout=0.0,conv_bias=False)\n",
    "print(\"num params: \", feature_extractor.params())\n",
    "print('input shape=',x.shape)\n",
    "conv_out = feature_extractor(x, verbose=True)\n",
    "print(\"output shape= \", conv_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 688, 512])\n"
     ]
    }
   ],
   "source": [
    "features = conv_out.transpose(1, 2) # transpose \n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 688, 512])\n"
     ]
    }
   ],
   "source": [
    "# regular layer norm\n",
    "ln = torch.nn.LayerNorm(dim, 1e-5, elementwise_affine=True)\n",
    "features = ln(features)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 688, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_input = nn.Dropout(0.0)\n",
    "dropout_features = nn.Dropout(0.0)\n",
    "\n",
    "features = dropout_input(features)\n",
    "unmasked_features = dropout_features(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_vars = 320\n",
    "temp_default = (2, 0.5, 0.999995)\n",
    "groups = 2\n",
    "vq_dim = 768 # output_dim? to match input of attn\n",
    "weight_proj_factor = 3\n",
    "gum = GumbelVectorQuantizer(\n",
    "    dim=dim,\n",
    "    num_vars=latent_vars,\n",
    "    temp=temp_default,\n",
    "    combine_groups=False,\n",
    "    groups=groups,\n",
    "    vq_dim=vq_dim,\n",
    "    time_first=True,\n",
    "    weight_proj_depth=1,\n",
    "    weight_proj_factor=weight_proj_factor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GumbelVectorQuantizer(\n",
       "  (weight_proj): Linear(in_features=512, out_features=640, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "q  = gum(features, produce_targets=False)\n",
    "\n",
    "features = q[\"x\"]\n",
    "num_vars = q[\"num_vars\"]\n",
    "code_ppl = q[\"code_perplexity\"]\n",
    "prob_ppl = q[\"prob_perplexity\"]\n",
    "curr_temp = q[\"temp\"]\n",
    "\n",
    "project_inp = nn.Linear(vq_dim,768)\n",
    "\n",
    "feature = project_inp(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 688, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/projects/jupyterlab/Notebooks/song2vec/venv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "from fairseq.models.wav2vec import ConformerEncoder\n",
    "from fairseq.models.wav2vec import Wav2Vec2Config\n",
    "\n",
    "cfg = Wav2Vec2Config()\n",
    "cfg.pos_enc_type = \"rope\"\n",
    "encoder = ConformerEncoder\n",
    "encoder = encoder(cfg)\n",
    "x, layer_results = encoder(features,padding_mask=None,layer=None)\n",
    "y = unmasked_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after transformer layer\n",
    "q = gum(y, produce_targets=False)\n",
    "y = q[\"x\"]\n",
    "num_vars = q[\"num_vars\"]\n",
    "code_ppl = q[\"code_perplexity\"]\n",
    "prob_ppl = q[\"prob_perplexity\"]\n",
    "curr_temp = q[\"temp\"]\n",
    "\n",
    "project_q = nn.Linear(768,768)\n",
    "y = project_q(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sample_negatives() missing 1 required positional argument: 'num'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sample_negatives\n\u001b[0;32m----> 3\u001b[0m negs, _ \u001b[38;5;241m=\u001b[39m \u001b[43msample_negatives\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: sample_negatives() missing 1 required positional argument: 'num'"
     ]
    }
   ],
   "source": [
    "from utils import sample_negatives\n",
    "\n",
    "negs, _ = sample_negatives(\n",
    "    y,\n",
    "    y.size(1),\n",
    "    padding_count=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FULL BREAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.models.wav2vec import Wav2Vec2Model\n",
    "from fairseq.models.wav2vec import Wav2Vec2Config\n",
    "\n",
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "#\n",
    "# This source code is licensed under the MIT license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import math\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from fairseq import utils\n",
    "from fairseq.data.data_utils import compute_mask_indices\n",
    "from fairseq.dataclass import ChoiceEnum, FairseqDataclass\n",
    "from fairseq.distributed import fsdp_wrap\n",
    "from fairseq.models import BaseFairseqModel, register_model\n",
    "from fairseq.modules import (\n",
    "    Fp32GroupNorm,\n",
    "    Fp32LayerNorm,\n",
    "    GradMultiply,\n",
    "    LayerNorm,\n",
    "    MultiheadAttention,\n",
    "    RelPositionalEncoding,\n",
    "    SamePad,\n",
    "    TransposeLast,\n",
    ")\n",
    "from fairseq.modules.checkpoint_activations import checkpoint_wrapper\n",
    "from fairseq.modules.conformer_layer import ConformerWav2Vec2EncoderLayer\n",
    "from fairseq.modules.transformer_sentence_encoder import init_bert_params\n",
    "from fairseq.utils import buffered_arange, index_put, is_xla_tensor\n",
    "\n",
    "\n",
    "EXTRACTOR_MODE_CHOICES = ChoiceEnum([\"default\", \"layer_norm\"])\n",
    "MASKING_DISTRIBUTION_CHOICES = ChoiceEnum([\"static\", \"uniform\", \"normal\", \"poisson\"])\n",
    "LAYER_TYPE_CHOICES = ChoiceEnum([\"transformer\", \"conformer\"])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Wav2Vec2Config(FairseqDataclass):\n",
    "    extractor_mode: EXTRACTOR_MODE_CHOICES = field(\n",
    "        default=\"default\",\n",
    "        metadata={\n",
    "            \"help\": \"mode for feature extractor. default has a single group norm with d \"\n",
    "            \"groups in the first conv block, whereas layer_norm has layer norms in \"\n",
    "            \"every block (meant to use with normalize=True)\"\n",
    "        },\n",
    "    )\n",
    "    encoder_layers: int = field(\n",
    "        default=12, metadata={\"help\": \"num encoder layers in the transformer\"}\n",
    "    )\n",
    "    encoder_embed_dim: int = field(\n",
    "        default=768, metadata={\"help\": \"encoder embedding dimension\"}\n",
    "    )\n",
    "    encoder_ffn_embed_dim: int = field(\n",
    "        default=3072, metadata={\"help\": \"encoder embedding dimension for FFN\"}\n",
    "    )\n",
    "    encoder_attention_heads: int = field(\n",
    "        default=12, metadata={\"help\": \"num encoder attention heads\"}\n",
    "    )\n",
    "    activation_fn: ChoiceEnum(utils.get_available_activation_fns()) = field(\n",
    "        default=\"gelu\", metadata={\"help\": \"activation function to use\"}\n",
    "    )\n",
    "    layer_type: LAYER_TYPE_CHOICES = field(\n",
    "        default=\"transformer\", metadata={\"help\": \"layer type in encoder\"}\n",
    "    )\n",
    "    # dropouts\n",
    "    dropout: float = field(\n",
    "        default=0.1, metadata={\"help\": \"dropout probability for the transformer\"}\n",
    "    )\n",
    "    attention_dropout: float = field(\n",
    "        default=0.1, metadata={\"help\": \"dropout probability for attention weights\"}\n",
    "    )\n",
    "    activation_dropout: float = field(\n",
    "        default=0.0, metadata={\"help\": \"dropout probability after activation in FFN\"}\n",
    "    )\n",
    "    encoder_layerdrop: float = field(\n",
    "        default=0.0, metadata={\"help\": \"probability of dropping a tarnsformer layer\"}\n",
    "    )\n",
    "    dropout_input: float = field(\n",
    "        default=0.0,\n",
    "        metadata={\"help\": \"dropout to apply to the input (after feat extr)\"},\n",
    "    )\n",
    "    dropout_features: float = field(\n",
    "        default=0.0,\n",
    "        metadata={\"help\": \"dropout to apply to the features (after feat extr)\"},\n",
    "    )\n",
    "\n",
    "    final_dim: int = field(\n",
    "        default=0,\n",
    "        metadata={\n",
    "            \"help\": \"project final representations and targets to this many dimensions.\"\n",
    "            \"set to encoder_embed_dim is <= 0\"\n",
    "        },\n",
    "    )\n",
    "    layer_norm_first: bool = field(\n",
    "        default=False, metadata={\"help\": \"apply layernorm first in the transformer\"}\n",
    "    )\n",
    "    conv_feature_layers: str = field(\n",
    "        default=\"[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]\",\n",
    "        metadata={\n",
    "            \"help\": \"string describing convolutional feature extraction layers in form of a python list that contains \"\n",
    "            \"[(dim, kernel_size, stride), ...]\"\n",
    "        },\n",
    "    )\n",
    "    conv_bias: bool = field(\n",
    "        default=False, metadata={\"help\": \"include bias in conv encoder\"}\n",
    "    )\n",
    "    logit_temp: float = field(\n",
    "        default=0.1, metadata={\"help\": \"temperature to divide logits by\"}\n",
    "    )\n",
    "    quantize_targets: bool = field(\n",
    "        default=False, metadata={\"help\": \"use quantized targets\"}\n",
    "    )\n",
    "    quantize_input: bool = field(\n",
    "        default=True, metadata={\"help\": \"use quantized inputs\"}\n",
    "    )\n",
    "    same_quantizer: bool = field(\n",
    "        default=False, metadata={\"help\": \"use same quantizer for inputs and targets\"}\n",
    "    )\n",
    "    target_glu: bool = field(\n",
    "        default=False, metadata={\"help\": \"adds projection + glu to targets\"}\n",
    "    )\n",
    "    feature_grad_mult: float = field(\n",
    "        default=1.0, metadata={\"help\": \"multiply feature extractor var grads by this\"}\n",
    "    )\n",
    "    quantizer_depth: int = field(\n",
    "        default=1,\n",
    "        metadata={\"help\": \"number of quantizer layers\"},\n",
    "    )\n",
    "    quantizer_factor: int = field(\n",
    "        default=3,\n",
    "        metadata={\n",
    "            \"help\": \"dimensionality increase for inner quantizer layers (if depth > 1)\"\n",
    "        },\n",
    "    )\n",
    "    latent_vars: int = field(\n",
    "        default=320,\n",
    "        metadata={\"help\": \"number of latent variables V in each group of the codebook\"},\n",
    "    )\n",
    "    latent_groups: int = field(\n",
    "        default=2,\n",
    "        metadata={\"help\": \"number of groups G of latent variables in the codebook\"},\n",
    "    )\n",
    "    latent_dim: int = field(\n",
    "        default=0,\n",
    "        metadata={\n",
    "            \"help\": \"if > 0, uses this dimensionality for latent variables. \"\n",
    "            \"otherwise uses final_dim / latent_groups\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # masking\n",
    "    mask_length: int = field(default=10, metadata={\"help\": \"mask length\"})\n",
    "    mask_prob: float = field(\n",
    "        default=0.65, metadata={\"help\": \"probability of replacing a token with mask\"}\n",
    "    )\n",
    "    mask_selection: MASKING_DISTRIBUTION_CHOICES = field(\n",
    "        default=\"static\", metadata={\"help\": \"how to choose mask length\"}\n",
    "    )\n",
    "    mask_other: float = field(\n",
    "        default=0,\n",
    "        metadata={\n",
    "            \"help\": \"secondary mask argument (used for more complex distributions), \"\n",
    "            \"see help in compute_mask_indices\"\n",
    "        },\n",
    "    )\n",
    "    no_mask_overlap: bool = field(\n",
    "        default=False, metadata={\"help\": \"whether to allow masks to overlap\"}\n",
    "    )\n",
    "    mask_min_space: int = field(\n",
    "        default=1,\n",
    "        metadata={\"help\": \"min space between spans (if no overlap is enabled)\"},\n",
    "    )\n",
    "    require_same_masks: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"whether to number of masked timesteps must be the same across all \"\n",
    "            \"examples in a batch\"\n",
    "        },\n",
    "    )\n",
    "    mask_dropout: float = field(\n",
    "        default=0.0,\n",
    "        metadata={\"help\": \"percent of masks to unmask for each sample\"},\n",
    "    )\n",
    "\n",
    "    # channel masking\n",
    "    mask_channel_length: int = field(\n",
    "        default=10, metadata={\"help\": \"length of the mask for features (channels)\"}\n",
    "    )\n",
    "    mask_channel_prob: float = field(\n",
    "        default=0.0, metadata={\"help\": \"probability of replacing a feature with 0\"}\n",
    "    )\n",
    "    mask_channel_before: bool = False\n",
    "    mask_channel_selection: MASKING_DISTRIBUTION_CHOICES = field(\n",
    "        default=\"static\",\n",
    "        metadata={\"help\": \"how to choose mask length for channel masking\"},\n",
    "    )\n",
    "    mask_channel_other: float = field(\n",
    "        default=0,\n",
    "        metadata={\n",
    "            \"help\": \"secondary mask argument (used for more complex distributions), \"\n",
    "            \"see help in compute_mask_indicesh\"\n",
    "        },\n",
    "    )\n",
    "    no_mask_channel_overlap: bool = field(\n",
    "        default=False, metadata={\"help\": \"whether to allow channel masks to overlap\"}\n",
    "    )\n",
    "    mask_channel_min_space: int = field(\n",
    "        default=1,\n",
    "        metadata={\"help\": \"min space between spans (if no overlap is enabled)\"},\n",
    "    )\n",
    "\n",
    "    # negative selection\n",
    "    num_negatives: int = field(\n",
    "        default=100,\n",
    "        metadata={\"help\": \"number of negative examples from the same sample\"},\n",
    "    )\n",
    "    negatives_from_everywhere: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"sample negatives from everywhere, not just masked states\"},\n",
    "    )\n",
    "    cross_sample_negatives: int = field(\n",
    "        default=0, metadata={\"help\": \"number of negative examples from the any sample\"}\n",
    "    )\n",
    "    codebook_negatives: int = field(\n",
    "        default=0, metadata={\"help\": \"number of negative examples codebook\"}\n",
    "    )\n",
    "\n",
    "    # positional embeddings\n",
    "    conv_pos: int = field(\n",
    "        default=128,\n",
    "        metadata={\"help\": \"number of filters for convolutional positional embeddings\"},\n",
    "    )\n",
    "    conv_pos_groups: int = field(\n",
    "        default=16,\n",
    "        metadata={\"help\": \"number of groups for convolutional positional embedding\"},\n",
    "    )\n",
    "    pos_conv_depth: int = field(\n",
    "        default=1,\n",
    "        metadata={\"help\": \"depth of positional encoder network\"},\n",
    "    )\n",
    "\n",
    "    latent_temp: Tuple[float, float, float] = field(\n",
    "        default=(2, 0.5, 0.999995),\n",
    "        metadata={\n",
    "            \"help\": \"temperature for latent variable sampling. \"\n",
    "            \"can be tuple of 3 values (start, end, decay)\"\n",
    "        },\n",
    "    )\n",
    "    max_positions: int = field(default=100000, metadata={\"help\": \"Max positions\"})\n",
    "    checkpoint_activations: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"recompute activations and save memory for extra compute\"},\n",
    "    )\n",
    "\n",
    "    # FP16 optimization\n",
    "    required_seq_len_multiple: int = field(\n",
    "        default=2,\n",
    "        metadata={\n",
    "            \"help\": \"pad the input to encoder such that the sequence length is divisible by multiple\"\n",
    "        },\n",
    "    )\n",
    "    crop_seq_to_multiple: int = field(\n",
    "        default=1,\n",
    "        metadata={\n",
    "            \"help\": \"crop convolutional feature extractor output such that the sequence length is divisible by multiple\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Conformer\n",
    "    depthwise_conv_kernel_size: int = field(\n",
    "        default=31,\n",
    "        metadata={\n",
    "            \"help\": \"depthwise-conv-kernel-size for convolution in conformer layer\"\n",
    "        },\n",
    "    )\n",
    "    attn_type: str = field(\n",
    "        default=\"\",\n",
    "        metadata={\"help\": \"if espnet use ESPNET MHA\"},\n",
    "    )\n",
    "    pos_enc_type: str = field(\n",
    "        default=\"abs\",  # must be ['real_pos' or 'rope' to use conformer]\n",
    "        metadata={\"help\": \"Positional encoding type to use in conformer\"},\n",
    "    )\n",
    "    fp16: bool = field(default=False, metadata={\"help\": \"If fp16 is being used\"})\n",
    "\n",
    "\n",
    "class Wav2Vec2Model(BaseFairseqModel):\n",
    "    def __init__(self, cfg: Wav2Vec2Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # default: [(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]\n",
    "        feature_enc_layers = eval(cfg.conv_feature_layers)\n",
    "\n",
    "        # default 512\n",
    "        self.embed = feature_enc_layers[-1][0]\n",
    "\n",
    "        self.feature_extractor = ConvFeatureExtractionModel(\n",
    "            conv_layers=feature_enc_layers,\n",
    "            dropout=0.0,\n",
    "            mode=cfg.extractor_mode,\n",
    "            conv_bias=cfg.conv_bias,\n",
    "        )\n",
    "\n",
    "        # default nn.Linear(512,768)\n",
    "        self.post_extract_proj = (\n",
    "            nn.Linear(\n",
    "                self.embed, cfg.encoder_embed_dim\n",
    "            )  # default cfg.encoder_embed_dim == 768\n",
    "            if self.embed != cfg.encoder_embed_dim\n",
    "            and not cfg.quantize_input  # default false\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        self.crop_seq_to_multiple = cfg.crop_seq_to_multiple  # 1\n",
    "\n",
    "        self.mask_prob = cfg.mask_prob  # 0.65\n",
    "        self.mask_selection = cfg.mask_selection  # \"static\"\n",
    "        self.mask_other = cfg.mask_other  # 0\n",
    "        self.mask_length = cfg.mask_length  # 10\n",
    "        self.no_mask_overlap = cfg.no_mask_overlap  # False\n",
    "        self.mask_min_space = cfg.mask_min_space  # 1\n",
    "\n",
    "        self.mask_channel_prob = cfg.mask_channel_prob  # 0.0\n",
    "        self.mask_channel_before = cfg.mask_channel_before  # False\n",
    "        self.mask_channel_selection = cfg.mask_channel_selection  # \"static\"\n",
    "        self.mask_channel_other = cfg.mask_channel_other  # 0\n",
    "        self.mask_channel_length = cfg.mask_channel_length  # 10\n",
    "        self.no_mask_channel_overlap = cfg.no_mask_channel_overlap  # False\n",
    "        self.mask_channel_min_space = cfg.mask_channel_min_space  # 1\n",
    "\n",
    "        self.dropout_input = nn.Dropout(cfg.dropout_input)  # 0.0\n",
    "        self.dropout_features = nn.Dropout(cfg.dropout_features)  # 0.0\n",
    "\n",
    "        self.feature_grad_mult = cfg.feature_grad_mult  # 1.0\n",
    "\n",
    "        self.quantizer = None\n",
    "        self.input_quantizer = None\n",
    "\n",
    "        self.n_negatives = cfg.num_negatives  # 100\n",
    "        self.cross_sample_negatives = cfg.cross_sample_negatives  # 0\n",
    "        self.codebook_negatives = cfg.codebook_negatives  # 0\n",
    "        self.negatives_from_everywhere = cfg.negatives_from_everywhere  # False\n",
    "\n",
    "        self.logit_temp = cfg.logit_temp  # 0.1\n",
    "\n",
    "        # final_dim is default 0\n",
    "        final_dim = cfg.final_dim if cfg.final_dim > 0 else cfg.encoder_embed_dim  # 768\n",
    "\n",
    "        # default to else\n",
    "        if cfg.quantize_targets:\n",
    "            vq_dim = (\n",
    "                cfg.latent_dim if cfg.latent_dim > 0 else final_dim\n",
    "            )  # default 0 -> final dim 0 -> 768\n",
    "            self.quantizer = GumbelVectorQuantizer(\n",
    "                dim=self.embed,  # 512\n",
    "                num_vars=cfg.latent_vars,  # 320\n",
    "                temp=cfg.latent_temp,  # (2, 0.5, 0.999995)\n",
    "                groups=cfg.latent_groups,  # 2\n",
    "                combine_groups=False,\n",
    "                vq_dim=vq_dim,  # 768\n",
    "                time_first=True,\n",
    "                weight_proj_depth=cfg.quantizer_depth,  # 1\n",
    "                weight_proj_factor=cfg.quantizer_factor,  # 3\n",
    "            )\n",
    "            self.project_q = nn.Linear(vq_dim, final_dim)  # (768,768)?\n",
    "        else:\n",
    "            # Default path\n",
    "            self.project_q = nn.Linear(self.embed, final_dim)  # (768,768)\n",
    "\n",
    "        # default false\n",
    "        if cfg.quantize_input:\n",
    "            if cfg.same_quantizer and self.quantizer is not None:\n",
    "                vq_dim = final_dim\n",
    "                self.input_quantizer = self.quantizer\n",
    "            else:\n",
    "                vq_dim = cfg.latent_dim if cfg.latent_dim > 0 else cfg.encoder_embed_dim\n",
    "                self.input_quantizer = GumbelVectorQuantizer(\n",
    "                    dim=self.embed,  # 512\n",
    "                    num_vars=cfg.latent_vars,  # 320\n",
    "                    temp=cfg.latent_temp,  # (2, 0.5, 0.999995)\n",
    "                    groups=cfg.latent_groups,  # 2\n",
    "                    combine_groups=False,\n",
    "                    vq_dim=vq_dim,\n",
    "                    time_first=True,\n",
    "                    weight_proj_depth=cfg.quantizer_depth,\n",
    "                    weight_proj_factor=cfg.quantizer_factor,\n",
    "                )\n",
    "            self.project_inp = nn.Linear(vq_dim, cfg.encoder_embed_dim)\n",
    "\n",
    "        self.mask_emb = nn.Parameter(\n",
    "            torch.FloatTensor(cfg.encoder_embed_dim).uniform_()  # just a parameter?\n",
    "        )\n",
    "        # default transformer\n",
    "        encoder_cls = TransformerEncoder\n",
    "        if cfg.layer_type == \"conformer\" and cfg.pos_enc_type in [\"rel_pos\", \"rope\"]:\n",
    "            encoder_cls = ConformerEncoder\n",
    "\n",
    "        self.encoder = encoder_cls(cfg)\n",
    "        self.layer_norm = LayerNorm(self.embed)  # 512 layer norm\n",
    "\n",
    "        self.target_glu = None\n",
    "        if cfg.target_glu:  # default false\n",
    "            self.target_glu = nn.Sequential(\n",
    "                nn.Linear(final_dim, final_dim * 2), nn.GLU()\n",
    "            )\n",
    "\n",
    "        self.final_proj = nn.Linear(cfg.encoder_embed_dim, final_dim)  # (768,768)\n",
    "\n",
    "    def upgrade_state_dict_named(self, state_dict, name):\n",
    "        super().upgrade_state_dict_named(state_dict, name)\n",
    "        \"\"\"Upgrade a (possibly old) state dict for new versions of fairseq.\"\"\"\n",
    "        return state_dict\n",
    "\n",
    "    @classmethod\n",
    "    def build_model(cls, cfg: Wav2Vec2Config, task=None):\n",
    "        \"\"\"Build a new model instance.\"\"\"\n",
    "\n",
    "        return cls(cfg)\n",
    "\n",
    "    def apply_mask(\n",
    "        self,\n",
    "        x,\n",
    "        padding_mask,\n",
    "        mask_indices=None,\n",
    "        mask_channel_indices=None,\n",
    "    ):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        if self.mask_channel_prob > 0 and self.mask_channel_before:\n",
    "            mask_channel_indices = compute_mask_indices(\n",
    "                (B, C),\n",
    "                None,\n",
    "                self.mask_channel_prob,\n",
    "                self.mask_channel_length,\n",
    "                self.mask_channel_selection,\n",
    "                self.mask_channel_other,\n",
    "                no_overlap=self.no_mask_channel_overlap,\n",
    "                min_space=self.mask_channel_min_space,\n",
    "            )\n",
    "            mask_channel_indices = (\n",
    "                torch.from_numpy(mask_channel_indices)\n",
    "                .to(x.device)\n",
    "                .unsqueeze(1)\n",
    "                .expand(-1, T, -1)\n",
    "            )\n",
    "            x[mask_channel_indices] = 0\n",
    "\n",
    "        if self.mask_prob > 0:\n",
    "            if mask_indices is None:\n",
    "                mask_indices = compute_mask_indices(\n",
    "                    (B, T),\n",
    "                    padding_mask,\n",
    "                    self.mask_prob,\n",
    "                    self.mask_length,\n",
    "                    self.mask_selection,\n",
    "                    self.mask_other,\n",
    "                    min_masks=2,\n",
    "                    no_overlap=self.no_mask_overlap,\n",
    "                    min_space=self.mask_min_space,\n",
    "                    require_same_masks=self.cfg.require_same_masks,\n",
    "                    mask_dropout=self.cfg.mask_dropout,\n",
    "                )\n",
    "                mask_indices = torch.from_numpy(mask_indices).to(x.device)\n",
    "            x = index_put(x, mask_indices, self.mask_emb)\n",
    "        else:\n",
    "            mask_indices = None\n",
    "\n",
    "        if self.mask_channel_prob > 0 and not self.mask_channel_before:\n",
    "            if mask_channel_indices is None:\n",
    "                mask_channel_indices = compute_mask_indices(\n",
    "                    (B, C),\n",
    "                    None,\n",
    "                    self.mask_channel_prob,\n",
    "                    self.mask_channel_length,\n",
    "                    self.mask_channel_selection,\n",
    "                    self.mask_channel_other,\n",
    "                    no_overlap=self.no_mask_channel_overlap,\n",
    "                    min_space=self.mask_channel_min_space,\n",
    "                )\n",
    "                mask_channel_indices = (\n",
    "                    torch.from_numpy(mask_channel_indices)\n",
    "                    .to(x.device)\n",
    "                    .unsqueeze(1)\n",
    "                    .expand(-1, T, -1)\n",
    "                )\n",
    "            x = index_put(x, mask_channel_indices, 0)\n",
    "\n",
    "        return x, mask_indices\n",
    "\n",
    "    def sample_negatives(self, y, num, padding_count=None):\n",
    "\n",
    "        if self.n_negatives == 0 and self.cross_sample_negatives == 0:\n",
    "            return y.new(0)\n",
    "\n",
    "        bsz, tsz, fsz = y.shape\n",
    "        y = y.view(-1, fsz)  # BTC => (BxT)C\n",
    "\n",
    "        # FIXME: what happens if padding_count is specified?\n",
    "        cross_high = tsz * bsz\n",
    "        high = tsz - (padding_count or 0)\n",
    "        with torch.no_grad():\n",
    "            assert high > 1, f\"{bsz,tsz,fsz}\"\n",
    "\n",
    "            if self.n_negatives > 0:\n",
    "                tszs = (\n",
    "                    buffered_arange(num)\n",
    "                    .unsqueeze(-1)\n",
    "                    .expand(-1, self.n_negatives)\n",
    "                    .flatten()\n",
    "                )\n",
    "\n",
    "                neg_idxs = torch.randint(\n",
    "                    low=0, high=high - 1, size=(bsz, self.n_negatives * num)\n",
    "                )\n",
    "                neg_idxs[neg_idxs >= tszs] += 1\n",
    "\n",
    "            if self.cross_sample_negatives > 0:\n",
    "                tszs = (\n",
    "                    buffered_arange(num)\n",
    "                    .unsqueeze(-1)\n",
    "                    .expand(-1, self.cross_sample_negatives)\n",
    "                    .flatten()\n",
    "                )\n",
    "\n",
    "                cross_neg_idxs = torch.randint(\n",
    "                    low=0,\n",
    "                    high=cross_high - 1,\n",
    "                    size=(bsz, self.cross_sample_negatives * num),\n",
    "                )\n",
    "                cross_neg_idxs[cross_neg_idxs >= tszs] += 1\n",
    "\n",
    "        if self.n_negatives > 0:\n",
    "            neg_idxs = neg_idxs + (torch.arange(bsz).unsqueeze(1) * high)\n",
    "        else:\n",
    "            neg_idxs = cross_neg_idxs\n",
    "\n",
    "        if self.cross_sample_negatives > 0 and self.n_negatives > 0:\n",
    "            neg_idxs = torch.cat([neg_idxs, cross_neg_idxs], dim=1)\n",
    "\n",
    "        negs = y[neg_idxs.view(-1)]\n",
    "        negs = negs.view(\n",
    "            bsz, num, self.n_negatives + self.cross_sample_negatives, fsz\n",
    "        ).permute(\n",
    "            2, 0, 1, 3\n",
    "        )  # to NxBxTxC\n",
    "        return negs, neg_idxs\n",
    "\n",
    "    def compute_preds(self, x, y, negatives):\n",
    "\n",
    "        neg_is_pos = (y == negatives).all(-1)\n",
    "        y = y.unsqueeze(0)\n",
    "        targets = torch.cat([y, negatives], dim=0)\n",
    "\n",
    "        logits = torch.cosine_similarity(x.float(), targets.float(), dim=-1)\n",
    "        logits = logits / self.logit_temp\n",
    "        logits = logits.type_as(x)\n",
    "\n",
    "        if is_xla_tensor(logits) or neg_is_pos.any():\n",
    "            if not hasattr(self, \"_inftensor\"):\n",
    "                fillval = -float(2**30)\n",
    "                self._inftensor = (\n",
    "                    torch.tensor(fillval).to(x.device)\n",
    "                    if is_xla_tensor(logits)\n",
    "                    else float(\"-inf\")\n",
    "                )\n",
    "            logits[1:] = index_put(logits[1:], neg_is_pos, self._inftensor)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def _get_feat_extract_output_lengths(self, input_lengths: torch.LongTensor):\n",
    "        \"\"\"\n",
    "        Computes the output length of the convolutional layers\n",
    "        \"\"\"\n",
    "\n",
    "        def _conv_out_length(input_length, kernel_size, stride):\n",
    "            return torch.floor((input_length - kernel_size) / stride + 1)\n",
    "\n",
    "        conv_cfg_list = eval(self.cfg.conv_feature_layers)\n",
    "\n",
    "        for i in range(len(conv_cfg_list)):\n",
    "            input_lengths = _conv_out_length(\n",
    "                input_lengths, conv_cfg_list[i][1], conv_cfg_list[i][2]\n",
    "            )\n",
    "\n",
    "        return input_lengths.to(torch.long)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        source,\n",
    "        padding_mask=None,\n",
    "        mask=True,\n",
    "        features_only=False,\n",
    "        layer=None,\n",
    "        mask_indices=None,\n",
    "        mask_channel_indices=None,\n",
    "        padding_count=None,\n",
    "    ):\n",
    "        \n",
    "        print(source.shape)\n",
    "\n",
    "        if self.feature_grad_mult > 0:\n",
    "            # default grad scaling, self.feature_grad_scaling == 1\n",
    "            features = self.feature_extractor(source)\n",
    "            if self.feature_grad_mult != 1.0:\n",
    "                features = GradMultiply.apply(features, self.feature_grad_mult)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                features = self.feature_extractor(source)\n",
    "\n",
    "\n",
    "        print(features.shape)\n",
    "        # square mean, regularization term for later\n",
    "        features_pen = features.float().pow(2).mean()\n",
    "\n",
    "        # reshape from B something something to  B something something\n",
    "        features = features.transpose(1, 2)\n",
    "        print('transpose to: ',features.shape)\n",
    "        # regular layer norm\n",
    "        features = self.layer_norm(features)\n",
    "        # copy for later\n",
    "        unmasked_features = features.clone()\n",
    "\n",
    "        ################# padding mask adjustments\n",
    "        if padding_mask is not None and padding_mask.any():\n",
    "            input_lengths = (1 - padding_mask.long()).sum(-1)\n",
    "            # apply conv formula to get real output_lengths\n",
    "            output_lengths = self._get_feat_extract_output_lengths(input_lengths)\n",
    "\n",
    "            padding_mask = torch.zeros(\n",
    "                features.shape[:2], dtype=features.dtype, device=features.device\n",
    "            )\n",
    "\n",
    "            # these two operations makes sure that all values\n",
    "            # before the output lengths indices are attended to\n",
    "            padding_mask[\n",
    "                (\n",
    "                    torch.arange(padding_mask.shape[0], device=padding_mask.device),\n",
    "                    output_lengths - 1,\n",
    "                )\n",
    "            ] = 1\n",
    "            padding_mask = (1 - padding_mask.flip([-1]).cumsum(-1).flip([-1])).bool()\n",
    "        else:\n",
    "            padding_mask = None\n",
    "        #################\n",
    "\n",
    "        ################\n",
    "        # crop size of batch to multiple of self.crop_seq_to_multiple\n",
    "        time_steps_to_drop = features.size(1) % self.crop_seq_to_multiple  # default 1\n",
    "        if time_steps_to_drop != 0:\n",
    "            features = features[:, :-time_steps_to_drop]\n",
    "            unmasked_features = unmasked_features[:, :-time_steps_to_drop]\n",
    "            if padding_mask is not None:\n",
    "                padding_mask = padding_mask[:, :-time_steps_to_drop]\n",
    "\n",
    "        # project features to new dimension to match subsequent layers\n",
    "        if self.post_extract_proj is not None:\n",
    "            features = self.post_extract_proj(features)\n",
    "\n",
    "\n",
    "        print('after post_extract_proj: ', features.shape)\n",
    "        features = self.dropout_input(features)\n",
    "        unmasked_features = self.dropout_features(unmasked_features)\n",
    "\n",
    "        num_vars = None\n",
    "        code_ppl = None\n",
    "        prob_ppl = None\n",
    "        curr_temp = None\n",
    "\n",
    "\n",
    "        if self.input_quantizer:\n",
    "            print('HERE')\n",
    "\n",
    "            print(self.input_quantizer)\n",
    "\n",
    "            q = self.input_quantizer(features, produce_targets=False)\n",
    "            \n",
    "            features = q[\"x\"]\n",
    "            print(features.shape)\n",
    "            num_vars = q[\"num_vars\"]\n",
    "            code_ppl = q[\"code_perplexity\"]\n",
    "            prob_ppl = q[\"prob_perplexity\"]\n",
    "            curr_temp = q[\"temp\"]\n",
    "            features = self.project_inp(features)\n",
    "\n",
    "        \n",
    "\n",
    "        if mask:\n",
    "            x, mask_indices = self.apply_mask(\n",
    "                features,\n",
    "                padding_mask,\n",
    "                mask_indices=mask_indices,\n",
    "                mask_channel_indices=mask_channel_indices,\n",
    "            )\n",
    "            if not is_xla_tensor(x) and mask_indices is not None:\n",
    "                # tpu-comment: reducing the size in a dynamic way causes\n",
    "                # too many recompilations on xla.\n",
    "                y = unmasked_features[mask_indices].view(\n",
    "                    unmasked_features.size(0), -1, unmasked_features.size(-1)\n",
    "                )\n",
    "            else:\n",
    "                y = unmasked_features\n",
    "        else:\n",
    "            x = features\n",
    "            y = unmasked_features\n",
    "            mask_indices = None\n",
    "\n",
    "        x, layer_results = self.encoder(x, padding_mask=padding_mask, layer=layer)\n",
    "\n",
    "        if features_only:\n",
    "            return {\n",
    "                \"x\": x,\n",
    "                \"padding_mask\": padding_mask,\n",
    "                \"features\": unmasked_features,\n",
    "                \"layer_results\": layer_results,\n",
    "            }\n",
    "\n",
    "        if self.quantizer:\n",
    "            if self.negatives_from_everywhere:\n",
    "                q = self.quantizer(unmasked_features, produce_targets=False)\n",
    "                y = q[\"x\"]\n",
    "                num_vars = q[\"num_vars\"]\n",
    "                code_ppl = q[\"code_perplexity\"]\n",
    "                prob_ppl = q[\"prob_perplexity\"]\n",
    "                curr_temp = q[\"temp\"]\n",
    "                y = self.project_q(y)\n",
    "\n",
    "                negs, _ = self.sample_negatives(\n",
    "                    y,\n",
    "                    mask_indices[0].sum(),\n",
    "                    padding_count=padding_count,\n",
    "                )\n",
    "                y = y[mask_indices].view(y.size(0), -1, y.size(-1))\n",
    "\n",
    "            else:\n",
    "                q = self.quantizer(y, produce_targets=False)\n",
    "                y = q[\"x\"]\n",
    "                num_vars = q[\"num_vars\"]\n",
    "                code_ppl = q[\"code_perplexity\"]\n",
    "                prob_ppl = q[\"prob_perplexity\"]\n",
    "                curr_temp = q[\"temp\"]\n",
    "\n",
    "                y = self.project_q(y)\n",
    "\n",
    "                negs, _ = self.sample_negatives(\n",
    "                    y,\n",
    "                    y.size(1),\n",
    "                    padding_count=padding_count,\n",
    "                )\n",
    "\n",
    "            if self.codebook_negatives > 0:\n",
    "                cb_negs = self.quantizer.sample_from_codebook(\n",
    "                    y.size(0) * y.size(1), self.codebook_negatives\n",
    "                )\n",
    "                cb_negs = cb_negs.view(\n",
    "                    self.codebook_negatives, y.size(0), y.size(1), -1\n",
    "                )  # order doesnt matter\n",
    "                cb_negs = self.project_q(cb_negs)\n",
    "                negs = torch.cat([negs, cb_negs], dim=0)\n",
    "        else:\n",
    "            y = self.project_q(y)\n",
    "\n",
    "            if self.negatives_from_everywhere:\n",
    "                negs, _ = self.sample_negatives(\n",
    "                    unmasked_features,\n",
    "                    y.size(1),\n",
    "                    padding_count=padding_count,\n",
    "                )\n",
    "                negs = self.project_q(negs)\n",
    "            else:\n",
    "                negs, _ = self.sample_negatives(\n",
    "                    y,\n",
    "                    y.size(1),\n",
    "                    padding_count=padding_count,\n",
    "                )\n",
    "\n",
    "        if not is_xla_tensor(x):\n",
    "            # tpu-comment: reducing the size in a dynamic way causes\n",
    "            # too many recompilations on xla.\n",
    "            x = x[mask_indices].view(x.size(0), -1, x.size(-1))\n",
    "\n",
    "        if self.target_glu:\n",
    "            y = self.target_glu(y)\n",
    "            negs = self.target_glu(negs)\n",
    "\n",
    "        x = self.final_proj(x)\n",
    "        x = self.compute_preds(x, y, negs)\n",
    "\n",
    "        result = {\n",
    "            \"x\": x,\n",
    "            \"padding_mask\": padding_mask,\n",
    "            \"features_pen\": features_pen,\n",
    "        }\n",
    "\n",
    "        if prob_ppl is not None:\n",
    "            result[\"prob_perplexity\"] = prob_ppl\n",
    "            result[\"code_perplexity\"] = code_ppl\n",
    "            result[\"num_vars\"] = num_vars\n",
    "            result[\"temp\"] = curr_temp\n",
    "\n",
    "        return result\n",
    "\n",
    "    def quantize(self, x):  # to be used on trained model? for inf?\n",
    "        assert self.quantizer is not None\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.layer_norm(x)\n",
    "        return self.quantizer.forward_idx(x)\n",
    "\n",
    "    def extract_features(self, source, padding_mask, mask=False, layer=None):\n",
    "        res = self.forward(\n",
    "            source, padding_mask, mask=mask, features_only=True, layer=layer\n",
    "        )\n",
    "        return res\n",
    "\n",
    "    def get_logits(self, net_output):\n",
    "        logits = net_output[\"x\"]\n",
    "        logits = logits.transpose(0, 2)\n",
    "        logits = logits.reshape(-1, logits.size(-1))\n",
    "        return logits\n",
    "\n",
    "    def get_targets(self, sample, net_output, expand_steps=True):\n",
    "        x = net_output[\"x\"]\n",
    "        return x.new_zeros(x.size(1) * x.size(2), dtype=torch.long)\n",
    "\n",
    "    def get_extra_losses(self, net_output):\n",
    "        pen = []\n",
    "\n",
    "        if \"prob_perplexity\" in net_output:\n",
    "            pen.append(\n",
    "                (net_output[\"num_vars\"] - net_output[\"prob_perplexity\"])\n",
    "                / net_output[\"num_vars\"]\n",
    "            )\n",
    "\n",
    "        if \"features_pen\" in net_output:\n",
    "            pen.append(net_output[\"features_pen\"])\n",
    "\n",
    "        return pen\n",
    "\n",
    "    def remove_pretraining_modules(self, last_layer=None):\n",
    "        self.quantizer = None\n",
    "        self.project_q = None\n",
    "        self.target_glu = None\n",
    "        self.final_proj = None\n",
    "\n",
    "        if last_layer is not None:\n",
    "            self.encoder.layers = nn.ModuleList(\n",
    "                l for i, l in enumerate(self.encoder.layers) if i <= last_layer\n",
    "            )\n",
    "\n",
    "\n",
    "class ConvFeatureExtractionModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        conv_layers: List[Tuple[int, int, int]],\n",
    "        dropout: float = 0.0,\n",
    "        mode: str = \"default\",\n",
    "        conv_bias: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        assert mode in {\"default\", \"layer_norm\"}\n",
    "\n",
    "        def block(\n",
    "            n_in,\n",
    "            n_out,\n",
    "            k,\n",
    "            stride,\n",
    "            is_layer_norm=False,\n",
    "            is_group_norm=False,\n",
    "            conv_bias=False,\n",
    "        ):\n",
    "            def make_conv():\n",
    "                conv = nn.Conv1d(n_in, n_out, k, stride=stride, bias=conv_bias)\n",
    "                nn.init.kaiming_normal_(conv.weight)\n",
    "                return conv\n",
    "\n",
    "            assert (\n",
    "                is_layer_norm and is_group_norm\n",
    "            ) == False, \"layer norm and group norm are exclusive\"\n",
    "\n",
    "            if is_layer_norm:\n",
    "                return nn.Sequential(\n",
    "                    make_conv(),\n",
    "                    nn.Dropout(p=dropout),\n",
    "                    nn.Sequential(\n",
    "                        TransposeLast(),\n",
    "                        Fp32LayerNorm(dim, elementwise_affine=True),\n",
    "                        TransposeLast(),\n",
    "                    ),\n",
    "                    nn.GELU(),\n",
    "                )\n",
    "            elif is_group_norm:\n",
    "                return nn.Sequential(\n",
    "                    make_conv(),\n",
    "                    nn.Dropout(p=dropout),\n",
    "                    Fp32GroupNorm(dim, dim, affine=True),\n",
    "                    nn.GELU(),\n",
    "                )\n",
    "            else:\n",
    "                return nn.Sequential(make_conv(), nn.Dropout(p=dropout), nn.GELU())\n",
    "\n",
    "        in_d = 1\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        for i, cl in enumerate(conv_layers):\n",
    "            assert len(cl) == 3, \"invalid conv definition: \" + str(cl)\n",
    "            (dim, k, stride) = cl\n",
    "\n",
    "            self.conv_layers.append(\n",
    "                block(\n",
    "                    in_d,\n",
    "                    dim,\n",
    "                    k,\n",
    "                    stride,\n",
    "                    is_layer_norm=mode == \"layer_norm\",\n",
    "                    is_group_norm=mode == \"default\" and i == 0,\n",
    "                    conv_bias=conv_bias,\n",
    "                )\n",
    "            )\n",
    "            in_d = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # BxT -> BxCxT\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        for conv in self.conv_layers:\n",
    "            x = conv(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def make_conv_pos(e, k, g):\n",
    "    pos_conv = nn.Conv1d(\n",
    "        e,\n",
    "        e,\n",
    "        kernel_size=k,\n",
    "        padding=k // 2,\n",
    "        groups=g,\n",
    "    )\n",
    "    dropout = 0\n",
    "    std = math.sqrt((4 * (1.0 - dropout)) / (k * e))\n",
    "    nn.init.normal_(pos_conv.weight, mean=0, std=std)\n",
    "    nn.init.constant_(pos_conv.bias, 0)\n",
    "\n",
    "    pos_conv = nn.utils.weight_norm(pos_conv, name=\"weight\", dim=2)\n",
    "    pos_conv = nn.Sequential(pos_conv, SamePad(k), nn.GELU())\n",
    "\n",
    "    return pos_conv\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def build_encoder_layer(self, args: Wav2Vec2Config):\n",
    "        if args.layer_type == \"transformer\":\n",
    "            layer = TransformerSentenceEncoderLayer(\n",
    "                embedding_dim=self.embedding_dim,\n",
    "                ffn_embedding_dim=args.encoder_ffn_embed_dim,\n",
    "                num_attention_heads=args.encoder_attention_heads,\n",
    "                dropout=self.dropout,\n",
    "                attention_dropout=args.attention_dropout,\n",
    "                activation_dropout=args.activation_dropout,\n",
    "                activation_fn=args.activation_fn,\n",
    "                layer_norm_first=args.layer_norm_first,\n",
    "            )\n",
    "        elif args.layer_type == \"conformer\":\n",
    "            layer = ConformerWav2Vec2EncoderLayer(\n",
    "                embed_dim=self.embedding_dim,\n",
    "                ffn_embed_dim=args.encoder_ffn_embed_dim,\n",
    "                attention_heads=args.encoder_attention_heads,\n",
    "                dropout=args.dropout,\n",
    "                depthwise_conv_kernel_size=args.depthwise_conv_kernel_size,\n",
    "                activation_fn=\"swish\",\n",
    "                attn_type=args.attn_type,\n",
    "                use_fp16=args.fp16,\n",
    "                pos_enc_type=\"abs\",\n",
    "            )\n",
    "        layer = fsdp_wrap(layer)\n",
    "        if args.checkpoint_activations:\n",
    "            layer = checkpoint_wrapper(layer)\n",
    "        return layer\n",
    "\n",
    "    def __init__(self, args: Wav2Vec2Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = args.dropout\n",
    "        self.embedding_dim = args.encoder_embed_dim\n",
    "        self.required_seq_len_multiple = args.required_seq_len_multiple\n",
    "\n",
    "        pos_conv_depth = getattr(args, \"pos_conv_depth\", 1)\n",
    "        if pos_conv_depth > 1:\n",
    "            num_layers = args.pos_conv_depth\n",
    "            k = max(3, args.conv_pos // num_layers)\n",
    "\n",
    "            def make_conv_block(e, k, g, l):\n",
    "                return nn.Sequential(\n",
    "                    *[\n",
    "                        nn.Sequential(\n",
    "                            nn.Conv1d(\n",
    "                                e,\n",
    "                                e,\n",
    "                                kernel_size=k,\n",
    "                                padding=k // 2,\n",
    "                                groups=g,\n",
    "                            ),\n",
    "                            SamePad(k),\n",
    "                            TransposeLast(),\n",
    "                            LayerNorm(e, elementwise_affine=False),\n",
    "                            TransposeLast(),\n",
    "                            nn.GELU(),\n",
    "                        )\n",
    "                        for _ in range(l)\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "            self.pos_conv = make_conv_block(\n",
    "                self.embedding_dim, k, args.conv_pos_groups, num_layers\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.pos_conv = make_conv_pos(\n",
    "                self.embedding_dim,\n",
    "                args.conv_pos,\n",
    "                args.conv_pos_groups,\n",
    "            )\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [self.build_encoder_layer(args) for _ in range(args.encoder_layers)]\n",
    "        )\n",
    "        self.layer_norm_first = args.layer_norm_first\n",
    "        self.layer_norm = LayerNorm(self.embedding_dim)\n",
    "        self.layerdrop = args.encoder_layerdrop\n",
    "\n",
    "        self.apply(init_bert_params)\n",
    "\n",
    "    def forward(self, x, padding_mask=None, layer=None):\n",
    "        x, layer_results = self.extract_features(x, padding_mask, layer)\n",
    "\n",
    "        if self.layer_norm_first and layer is None:\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "        return x, layer_results\n",
    "\n",
    "    def extract_features(\n",
    "        self,\n",
    "        x,\n",
    "        padding_mask=None,\n",
    "        tgt_layer=None,\n",
    "        min_layer=0,\n",
    "    ):\n",
    "\n",
    "        if padding_mask is not None:\n",
    "            x = index_put(x, padding_mask, 0)\n",
    "\n",
    "        x_conv = self.pos_conv(x.transpose(1, 2))\n",
    "        x_conv = x_conv.transpose(1, 2)\n",
    "        x = x + x_conv\n",
    "\n",
    "        if not self.layer_norm_first:\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "        # pad to the sequence length dimension\n",
    "        x, pad_length = pad_to_multiple(\n",
    "            x, self.required_seq_len_multiple, dim=-2, value=0\n",
    "        )\n",
    "        if pad_length > 0 and padding_mask is None:\n",
    "            padding_mask = x.new_zeros((x.size(0), x.size(1)), dtype=torch.bool)\n",
    "            padding_mask[:, -pad_length:] = True\n",
    "        else:\n",
    "            padding_mask, _ = pad_to_multiple(\n",
    "                padding_mask, self.required_seq_len_multiple, dim=-1, value=True\n",
    "            )\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # B x T x C -> T x B x C\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        layer_results = []\n",
    "        r = None\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            dropout_probability = np.random.random() if self.layerdrop > 0 else 1\n",
    "            if not self.training or (dropout_probability > self.layerdrop):\n",
    "                x, (z, lr) = layer(\n",
    "                    x, self_attn_padding_mask=padding_mask, need_weights=False\n",
    "                )\n",
    "                if i >= min_layer:\n",
    "                    layer_results.append((x, z, lr))\n",
    "            if i == tgt_layer:\n",
    "                r = x\n",
    "                break\n",
    "\n",
    "        if r is not None:\n",
    "            x = r\n",
    "\n",
    "        # T x B x C -> B x T x C\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        # undo paddding\n",
    "        if pad_length > 0:\n",
    "            x = x[:, :-pad_length]\n",
    "\n",
    "            def undo_pad(a, b, c):\n",
    "                return (\n",
    "                    a[:-pad_length],\n",
    "                    b[:-pad_length] if b is not None else b,\n",
    "                    c[:-pad_length],\n",
    "                )\n",
    "\n",
    "            layer_results = [undo_pad(*u) for u in layer_results]\n",
    "\n",
    "        return x, layer_results\n",
    "\n",
    "    def max_positions(self):\n",
    "        \"\"\"Maximum output length supported by the encoder.\"\"\"\n",
    "        return self.args.max_positions\n",
    "\n",
    "    def upgrade_state_dict_named(self, state_dict, name):\n",
    "        \"\"\"Upgrade a (possibly old) state dict for new versions of fairseq.\"\"\"\n",
    "        return state_dict\n",
    "\n",
    "\n",
    "class ConformerEncoder(TransformerEncoder):\n",
    "    def build_encoder_layer(self, args):\n",
    "        layer = ConformerWav2Vec2EncoderLayer(\n",
    "            embed_dim=self.embedding_dim,\n",
    "            ffn_embed_dim=args.encoder_ffn_embed_dim,\n",
    "            attention_heads=args.encoder_attention_heads,\n",
    "            dropout=args.dropout,\n",
    "            depthwise_conv_kernel_size=args.depthwise_conv_kernel_size,\n",
    "            activation_fn=\"swish\",\n",
    "            attn_type=args.attn_type,\n",
    "            pos_enc_type=args.pos_enc_type,\n",
    "            use_fp16=args.fp16,  # only used for rope\n",
    "        )\n",
    "        layer = fsdp_wrap(layer)\n",
    "        if args.checkpoint_activations:\n",
    "            layer = checkpoint_wrapper(layer)\n",
    "        return layer\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super().__init__(args)\n",
    "        self.args = args\n",
    "        self.dropout = args.dropout\n",
    "        self.embedding_dim = args.encoder_embed_dim\n",
    "        self.pos_enc_type = args.pos_enc_type\n",
    "        max_source_positions = self.max_positions()\n",
    "\n",
    "        if self.pos_enc_type == \"rel_pos\":\n",
    "            self.embed_positions = RelPositionalEncoding(\n",
    "                max_source_positions, self.embedding_dim\n",
    "            )\n",
    "        elif self.pos_enc_type == \"rope\":\n",
    "            self.embed_positions = None\n",
    "        else:\n",
    "            raise Exception(\"Unsupported positional encoding type\")\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [self.build_encoder_layer(args) for _ in range(args.encoder_layers)]\n",
    "        )\n",
    "        self.layer_norm_first = args.layer_norm_first\n",
    "        self.layer_norm = LayerNorm(self.embedding_dim)\n",
    "        self.layerdrop = args.encoder_layerdrop\n",
    "\n",
    "        self.apply(init_bert_params)\n",
    "\n",
    "    def extract_features(self, x, padding_mask=None, tgt_layer=None):\n",
    "        if padding_mask is not None:\n",
    "            x = index_put(x, padding_mask, 0)\n",
    "\n",
    "        # B x T x C -> T x B x C\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        # B X T X C here\n",
    "        position_emb = None\n",
    "        if self.pos_enc_type == \"rel_pos\":\n",
    "            position_emb = self.embed_positions(x)\n",
    "\n",
    "        if not self.layer_norm_first:\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        layer_results = []\n",
    "        r = None\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            dropout_probability = np.random.random()\n",
    "            if not self.training or (dropout_probability > self.layerdrop):\n",
    "                x, z = layer(\n",
    "                    x,\n",
    "                    self_attn_padding_mask=padding_mask,\n",
    "                    need_weights=False,\n",
    "                    position_emb=position_emb,\n",
    "                )\n",
    "                if tgt_layer is not None:\n",
    "                    layer_results.append((x, z))\n",
    "            if i == tgt_layer:\n",
    "                r = x\n",
    "                break\n",
    "\n",
    "        if r is not None:\n",
    "            x = r\n",
    "\n",
    "        # T x B x C -> B x T x C\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        return x, layer_results\n",
    "\n",
    "\n",
    "class TransformerSentenceEncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a Transformer Encoder Layer used in BERT/XLM style pre-trained\n",
    "    models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: float = 768,\n",
    "        ffn_embedding_dim: float = 3072,\n",
    "        num_attention_heads: int = 8,\n",
    "        dropout: float = 0.1,\n",
    "        attention_dropout: float = 0.1,\n",
    "        activation_dropout: float = 0.1,\n",
    "        activation_fn: str = \"relu\",\n",
    "        layer_norm_first: bool = False,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        # Initialize parameters\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout = dropout\n",
    "        self.activation_dropout = activation_dropout\n",
    "\n",
    "        # Initialize blocks\n",
    "        self.activation_fn = utils.get_activation_fn(activation_fn)\n",
    "        self.self_attn = MultiheadAttention(\n",
    "            self.embedding_dim,\n",
    "            num_attention_heads,\n",
    "            dropout=attention_dropout,\n",
    "            self_attention=True,\n",
    "        )\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(self.activation_dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.layer_norm_first = layer_norm_first\n",
    "\n",
    "        # layer norm associated with the self attention layer\n",
    "        self.self_attn_layer_norm = LayerNorm(self.embedding_dim)\n",
    "        self.fc1 = nn.Linear(self.embedding_dim, ffn_embedding_dim)\n",
    "        self.fc2 = nn.Linear(ffn_embedding_dim, self.embedding_dim)\n",
    "\n",
    "        # layer norm associated with the position wise feed-forward NN\n",
    "        self.final_layer_norm = LayerNorm(self.embedding_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        self_attn_mask: torch.Tensor = None,\n",
    "        self_attn_padding_mask: torch.Tensor = None,\n",
    "        need_weights: bool = False,\n",
    "        att_args=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        LayerNorm is applied either before or after the self-attention/ffn\n",
    "        modules similar to the original Transformer imlementation.\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "\n",
    "        if self.layer_norm_first:\n",
    "            x = self.self_attn_layer_norm(x)\n",
    "            x, attn = self.self_attn(\n",
    "                query=x,\n",
    "                key=x,\n",
    "                value=x,\n",
    "                key_padding_mask=self_attn_padding_mask,\n",
    "                attn_mask=self_attn_mask,\n",
    "                need_weights=False,\n",
    "            )\n",
    "            x = self.dropout1(x)\n",
    "            x = residual + x\n",
    "\n",
    "            residual = x\n",
    "            x = self.final_layer_norm(x)\n",
    "            x = self.activation_fn(self.fc1(x))\n",
    "            x = self.dropout2(x)\n",
    "            x = self.fc2(x)\n",
    "\n",
    "            layer_result = x\n",
    "\n",
    "            x = self.dropout3(x)\n",
    "            x = residual + x\n",
    "        else:\n",
    "            x, attn = self.self_attn(\n",
    "                query=x,\n",
    "                key=x,\n",
    "                value=x,\n",
    "                key_padding_mask=self_attn_padding_mask,\n",
    "                need_weights=False,\n",
    "            )\n",
    "\n",
    "            x = self.dropout1(x)\n",
    "            x = residual + x\n",
    "\n",
    "            x = self.self_attn_layer_norm(x)\n",
    "\n",
    "            residual = x\n",
    "            x = self.activation_fn(self.fc1(x))\n",
    "            x = self.dropout2(x)\n",
    "            x = self.fc2(x)\n",
    "\n",
    "            layer_result = x\n",
    "\n",
    "            x = self.dropout3(x)\n",
    "            x = residual + x\n",
    "            x = self.final_layer_norm(x)\n",
    "\n",
    "        return x, (attn, layer_result)\n",
    "\n",
    "\n",
    "class GumbelVectorQuantizer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_vars,\n",
    "        temp,\n",
    "        groups,\n",
    "        combine_groups,\n",
    "        vq_dim,\n",
    "        time_first,\n",
    "        activation=nn.GELU(),\n",
    "        weight_proj_depth=1,\n",
    "        weight_proj_factor=1,\n",
    "    ):\n",
    "        \"\"\"Vector quantization using gumbel softmax\n",
    "\n",
    "        Args:\n",
    "            dim: input dimension (channels)\n",
    "            num_vars: number of quantized vectors per group\n",
    "            temp: temperature for training. this should be a tuple of 3 elements: (start, stop, decay factor)\n",
    "            groups: number of groups for vector quantization\n",
    "            combine_groups: whether to use the vectors for all groups,\n",
    "                            concat the code book to make 1 larger codebook, for experimenting with config\n",
    "            vq_dim: dimensionality of the resulting quantized vector\n",
    "            time_first: if true, expect input in BxTxC format, otherwise in BxCxT\n",
    "            activation: what activation to use (should be a module). this is only used if weight_proj_depth is > 1\n",
    "            weight_proj_depth: number of layers (with activation in between) to project input before computing logits\n",
    "            weight_proj_factor: this is used only if weight_proj_depth is > 1. scales the inner dimensionality of\n",
    "                                projections by this factor\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.groups = groups\n",
    "        self.combine_groups = combine_groups\n",
    "        self.input_dim = dim\n",
    "        self.num_vars = num_vars\n",
    "        self.time_first = time_first\n",
    "\n",
    "        assert (\n",
    "            vq_dim % groups == 0\n",
    "        ), f\"dim {vq_dim} must be divisible by groups {groups} for concatenation\"\n",
    "\n",
    "        var_dim = vq_dim // groups\n",
    "        num_groups = groups if not combine_groups else 1\n",
    "\n",
    "        self.vars = nn.Parameter(torch.FloatTensor(1, num_groups * num_vars, var_dim))\n",
    "        nn.init.uniform_(self.vars)\n",
    "\n",
    "        if weight_proj_depth > 1:\n",
    "\n",
    "            def block(input_dim, output_dim):\n",
    "                return nn.Sequential(nn.Linear(input_dim, output_dim), activation)\n",
    "\n",
    "            inner_dim = self.input_dim * weight_proj_factor\n",
    "            self.weight_proj = nn.Sequential(\n",
    "                *[\n",
    "                    block(self.input_dim if i == 0 else inner_dim, inner_dim)\n",
    "                    for i in range(weight_proj_depth - 1)\n",
    "                ],\n",
    "                nn.Linear(inner_dim, groups * num_vars),\n",
    "            )\n",
    "        else:\n",
    "            self.weight_proj = nn.Linear(self.input_dim, groups * num_vars)\n",
    "            nn.init.normal_(self.weight_proj.weight, mean=0, std=1)\n",
    "            nn.init.zeros_(self.weight_proj.bias)\n",
    "\n",
    "        if isinstance(temp, str):\n",
    "            import ast\n",
    "\n",
    "            temp = ast.literal_eval(temp)\n",
    "        assert len(temp) == 3, f\"{temp}, {len(temp)}\"\n",
    "\n",
    "        self.max_temp, self.min_temp, self.temp_decay = temp\n",
    "        self.curr_temp = self.max_temp\n",
    "        self.codebook_indices = None\n",
    "\n",
    "    def set_num_updates(self, num_updates):\n",
    "        self.curr_temp = max(\n",
    "            self.max_temp * self.temp_decay**num_updates, self.min_temp\n",
    "        )\n",
    "\n",
    "    def get_codebook_indices(self):\n",
    "        if self.codebook_indices is None:\n",
    "            from itertools import product\n",
    "\n",
    "            p = [range(self.num_vars)] * self.groups\n",
    "            inds = list(product(*p))\n",
    "            self.codebook_indices = torch.tensor(\n",
    "                inds, dtype=torch.long, device=self.vars.device\n",
    "            ).flatten()\n",
    "\n",
    "            if not self.combine_groups:\n",
    "                self.codebook_indices = self.codebook_indices.view(\n",
    "                    self.num_vars**self.groups, -1\n",
    "                )\n",
    "                for b in range(1, self.groups):\n",
    "                    self.codebook_indices[:, b] += self.num_vars * b\n",
    "                self.codebook_indices = self.codebook_indices.flatten()\n",
    "        return self.codebook_indices\n",
    "\n",
    "    def codebook(self):\n",
    "        indices = self.get_codebook_indices()\n",
    "        return (\n",
    "            self.vars.squeeze(0)\n",
    "            .index_select(0, indices)\n",
    "            .view(self.num_vars**self.groups, -1)\n",
    "        )\n",
    "\n",
    "    def sample_from_codebook(self, b, n):\n",
    "        indices = self.get_codebook_indices()\n",
    "        indices = indices.view(-1, self.groups)\n",
    "        cb_size = indices.size(0)\n",
    "        assert (\n",
    "            n < cb_size\n",
    "        ), f\"sample size {n} is greater than size of codebook {cb_size}\"\n",
    "        sample_idx = torch.randint(low=0, high=cb_size, size=(b * n,))\n",
    "        indices = indices[sample_idx]\n",
    "\n",
    "        z = self.vars.squeeze(0).index_select(0, indices.flatten()).view(b, n, -1)\n",
    "        return z\n",
    "\n",
    "    def to_codebook_index(self, indices):\n",
    "        res = indices.new_full(indices.shape[:-1], 0)\n",
    "        for i in range(self.groups):\n",
    "            exponent = self.groups - i - 1\n",
    "            res += indices[..., i] * (self.num_vars**exponent)\n",
    "        return res\n",
    "\n",
    "    def forward_idx(self, x):\n",
    "        res = self.forward(x, produce_targets=True)\n",
    "        return res[\"x\"], res[\"targets\"]\n",
    "\n",
    "    def forward(self, x, produce_targets=False):\n",
    "        # print(x.shape)\n",
    "\n",
    "        result = {\"num_vars\": self.num_vars * self.groups}\n",
    "\n",
    "        if not self.time_first:\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "        bsz, tsz, fsz = x.shape\n",
    "        x = x.reshape(-1, fsz)\n",
    "        x = self.weight_proj(x)\n",
    "        x = x.view(bsz * tsz * self.groups, -1)\n",
    "\n",
    "        _, k = x.max(-1)\n",
    "        hard_x = (\n",
    "            x.new_zeros(*x.shape)\n",
    "            .scatter_(-1, k.view(-1, 1), 1.0)\n",
    "            .view(bsz * tsz, self.groups, -1)\n",
    "        )\n",
    "        hard_probs = torch.mean(hard_x.float(), dim=0)\n",
    "        result[\"code_perplexity\"] = torch.exp(\n",
    "            -torch.sum(hard_probs * torch.log(hard_probs + 1e-7), dim=-1)\n",
    "        ).sum()\n",
    "\n",
    "        avg_probs = torch.softmax(\n",
    "            x.view(bsz * tsz, self.groups, -1).float(), dim=-1\n",
    "        ).mean(dim=0)\n",
    "        result[\"prob_perplexity\"] = torch.exp(\n",
    "            -torch.sum(avg_probs * torch.log(avg_probs + 1e-7), dim=-1)\n",
    "        ).sum()\n",
    "\n",
    "        result[\"temp\"] = self.curr_temp\n",
    "\n",
    "        if self.training:\n",
    "            x = F.gumbel_softmax(x.float(), tau=self.curr_temp, hard=True).type_as(x)\n",
    "        else:\n",
    "            x = hard_x\n",
    "\n",
    "        x = x.view(bsz * tsz, -1)\n",
    "\n",
    "        vars = self.vars\n",
    "        if self.combine_groups:\n",
    "            vars = vars.repeat(1, self.groups, 1)\n",
    "\n",
    "        if produce_targets:\n",
    "            result[\"targets\"] = (\n",
    "                x.view(bsz * tsz * self.groups, -1)\n",
    "                .argmax(dim=-1)\n",
    "                .view(bsz, tsz, self.groups)\n",
    "                .detach()\n",
    "            )\n",
    "\n",
    "        x = x.unsqueeze(-1) * vars\n",
    "        x = x.view(bsz * tsz, self.groups, self.num_vars, -1)\n",
    "        x = x.sum(-2)\n",
    "        x = x.view(bsz, tsz, -1)\n",
    "\n",
    "        if not self.time_first:\n",
    "            x = x.transpose(1, 2)  # BTC -> BCT\n",
    "\n",
    "        result[\"x\"] = x\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "def pad_to_multiple(x, multiple, dim=-1, value=0):\n",
    "    # Inspired from https://github.com/lucidrains/local-attention/blob/master/local_attention/local_attention.py#L41\n",
    "    if x is None:\n",
    "        return None, 0\n",
    "    tsz = x.size(dim)\n",
    "    m = tsz / multiple\n",
    "    remainder = math.ceil(m) * multiple - tsz\n",
    "    if m.is_integer():\n",
    "        return x, 0\n",
    "    pad_offset = (0,) * (-1 - dim) * 2\n",
    "\n",
    "    return F.pad(x, (*pad_offset, 0, remainder), value=value), remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/projects/jupyterlab/Notebooks/song2vec/venv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2Model(Wav2Vec2Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 220500])\n",
      "torch.Size([2, 512, 688])\n",
      "transpose to:  torch.Size([2, 688, 512])\n",
      "after post_extract_proj:  torch.Size([2, 688, 512])\n",
      "HERE\n",
      "GumbelVectorQuantizer(\n",
      "  (weight_proj): Linear(in_features=512, out_features=640, bias=True)\n",
      ")\n",
      "torch.Size([2, 688, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'x': tensor([[[-0.0179,  0.0819,  0.1119,  ..., -0.1618,  0.1279, -0.0010],\n",
       "          [-0.0839,  0.0219,  0.0922,  ...,  0.3565, -0.0066,  0.7396]],\n",
       " \n",
       "         [[ 0.0611,  0.4111, -0.3843,  ...,  0.7387,  0.4068, -0.1616],\n",
       "          [ 0.1956,  0.1932,  0.1722,  ...,  0.0846, -0.3080, -0.1069]],\n",
       " \n",
       "         [[-0.3744,  0.0844,  0.5673,  ...,  0.1150, -0.1864, -0.1083],\n",
       "          [-0.4207,  0.3893, -0.3123,  ...,  0.0983, -0.0626,  0.2844]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.5721,  0.6294,  0.2648,  ..., -0.1269,  0.2497, -0.0790],\n",
       "          [ 0.2358,  0.4504,  0.4584,  ..., -0.1597, -0.4355, -0.2159]],\n",
       " \n",
       "         [[-0.1999, -0.2732,  0.0827,  ...,  0.3201,  0.2005, -0.3265],\n",
       "          [ 0.0407, -0.0120,  0.1026,  ...,  0.1102,  0.0369,  0.4956]],\n",
       " \n",
       "         [[ 0.2267,  0.5781,  0.3384,  ..., -0.1562, -0.1859, -0.1063],\n",
       "          [ 0.4431, -0.2727, -0.1210,  ..., -0.4235, -0.3137, -0.1809]]],\n",
       "        grad_fn=<DivBackward0>),\n",
       " 'padding_mask': None,\n",
       " 'features_pen': tensor(0.3284, grad_fn=<MeanBackward0>),\n",
       " 'prob_perplexity': tensor(294.9540, grad_fn=<SumBackward0>),\n",
       " 'code_perplexity': tensor(255.8471),\n",
       " 'num_vars': 640,\n",
       " 'temp': 2}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
