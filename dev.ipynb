{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from utils import *\n",
    "from feature_encoder import ConvFeatureExtractionModel\n",
    "from gumbel import GumbelVectorQuantizer\n",
    "import math\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 220500])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = []\n",
    "for i in range(1,3):\n",
    "    path = f\"data/mp3_train_files/Gould/Gould - WTC_clip_{i}.mp3\"\n",
    "    waveform,sample_rate = torchaudio.load(path)\n",
    "    waveform = torch.mean(waveform, dim=0).unsqueeze(0)\n",
    "    waveform = normalize_tensor(waveform)\n",
    "    x.append(waveform)\n",
    "\n",
    "\n",
    "x = torch.cat(x)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_waveform(x, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "receptive_field=400 samples\n",
      "9.07 ms\n"
     ]
    }
   ],
   "source": [
    "dim = 512 # quadratic to number of params in conv... double dim quadruple params\n",
    "\n",
    "conv_feature_layers = (\n",
    "    [(dim, 10, 2)] * 3 + [(dim, 3, 2)] * 5 + [(dim, 2, 2)] + [(dim, 2, 2)]\n",
    ")  # (dim,kernel width, strides) # kernel_width is window, string is len of slide\n",
    "conv_feature_layers = [(dim, 10, 5)] + [(dim, 3, 2)] * 4 + [(dim, 2, 2)] + [(dim, 2, 2)]\n",
    "receptive_field = calculate_receptive_field(conv_feature_layers)\n",
    "print(f\"{receptive_field=} samples\")\n",
    "print(f\"{round(receptive_field / (sample_rate / 1000),3)} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num params:  4200448\n",
      "input shape= torch.Size([2, 220500])\n",
      "torch.Size([2, 1, 220500])\n",
      "torch.Size([2, 512, 44099])\n",
      "torch.Size([2, 512, 22049])\n",
      "torch.Size([2, 512, 11024])\n",
      "torch.Size([2, 512, 5511])\n",
      "torch.Size([2, 512, 2755])\n",
      "torch.Size([2, 512, 1377])\n",
      "torch.Size([2, 512, 688])\n",
      "output shape=  torch.Size([2, 512, 688])\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = ConvFeatureExtractionModel(conv_feature_layers, mode=\"default\",dropout=0.0,conv_bias=False)\n",
    "print(\"num params: \", feature_extractor.params())\n",
    "print('input shape=',x.shape)\n",
    "conv_out = feature_extractor(x, verbose=True)\n",
    "print(\"output shape= \", conv_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 688, 512])\n"
     ]
    }
   ],
   "source": [
    "features = conv_out.transpose(1, 2) # transpose \n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 688, 512])\n"
     ]
    }
   ],
   "source": [
    "# regular layer norm\n",
    "ln = torch.nn.LayerNorm(dim, 1e-5, elementwise_affine=True)\n",
    "features = ln(features)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 688, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_input = nn.Dropout(0.0)\n",
    "dropout_features = nn.Dropout(0.0)\n",
    "\n",
    "features = dropout_input(features)\n",
    "unmasked_features = dropout_features(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_vars = 320\n",
    "temp_default = (2, 0.5, 0.999995)\n",
    "groups = 2\n",
    "vq_dim = 768 # output_dim? to match input of attn\n",
    "weight_proj_factor = 3\n",
    "gum = GumbelVectorQuantizer(\n",
    "    dim=dim,\n",
    "    num_vars=latent_vars,\n",
    "    temp=temp_default,\n",
    "    combine_groups=False,\n",
    "    groups=groups,\n",
    "    vq_dim=vq_dim,\n",
    "    time_first=True,\n",
    "    weight_proj_depth=1,\n",
    "    weight_proj_factor=weight_proj_factor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GumbelVectorQuantizer(\n",
       "  (weight_proj): Linear(in_features=512, out_features=640, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "q  = gum(features, produce_targets=False)\n",
    "\n",
    "features = q[\"x\"]\n",
    "num_vars = q[\"num_vars\"]\n",
    "code_ppl = q[\"code_perplexity\"]\n",
    "prob_ppl = q[\"prob_perplexity\"]\n",
    "curr_temp = q[\"temp\"]\n",
    "\n",
    "project_inp = nn.Linear(vq_dim,768)\n",
    "\n",
    "feature = project_inp(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 688, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/projects/jupyterlab/Notebooks/song2vec/venv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "from fairseq.models.wav2vec import ConformerEncoder\n",
    "from fairseq.models.wav2vec import Wav2Vec2Config\n",
    "\n",
    "cfg = Wav2Vec2Config()\n",
    "cfg.pos_enc_type = \"rope\"\n",
    "encoder = ConformerEncoder\n",
    "encoder = encoder(cfg)\n",
    "x, layer_results = encoder(features,padding_mask=None,layer=None)\n",
    "y = unmasked_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after transformer layer\n",
    "q = gum(y, produce_targets=False)\n",
    "y = q[\"x\"]\n",
    "num_vars = q[\"num_vars\"]\n",
    "code_ppl = q[\"code_perplexity\"]\n",
    "prob_ppl = q[\"prob_perplexity\"]\n",
    "curr_temp = q[\"temp\"]\n",
    "\n",
    "project_q = nn.Linear(768,768)\n",
    "y = project_q(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sample_negatives() missing 1 required positional argument: 'num'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sample_negatives\n\u001b[0;32m----> 3\u001b[0m negs, _ \u001b[38;5;241m=\u001b[39m \u001b[43msample_negatives\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: sample_negatives() missing 1 required positional argument: 'num'"
     ]
    }
   ],
   "source": [
    "from utils import sample_negatives\n",
    "\n",
    "negs, _ = sample_negatives(\n",
    "    y,\n",
    "    y.size(1),\n",
    "    padding_count=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/projects/jupyterlab/Notebooks/song2vec/venv/lib/python3.10/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoFeatureExtractor, Wav2Vec2ForPreTraining,Wav2Vec2Config\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    _compute_mask_indices,\n",
    "    _sample_negative_indices,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "\n",
    "\n",
    "config = Wav2Vec2Config()\n",
    "config.activation_dropout = 0.101\n",
    "\n",
    "model = Wav2Vec2ForPreTraining(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForPreTraining(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2GroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2Encoder(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.101, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout_features): Dropout(p=0.0, inplace=False)\n",
       "  (quantizer): Wav2Vec2GumbelVectorQuantizer(\n",
       "    (weight_proj): Linear(in_features=512, out_features=640, bias=True)\n",
       "  )\n",
       "  (project_hid): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (project_q): Linear(in_features=256, out_features=256, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/projects/jupyterlab/Notebooks/song2vec/venv/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for hf-internal-testing/librispeech_asr_dummy contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hf-internal-testing/librispeech_asr_dummy\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\n",
    "    \"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\"\n",
    ")\n",
    "input_values = feature_extractor(\n",
    "    ds[0][\"audio\"][\"array\"], return_tensors=\"pt\"\n",
    ").input_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, raw_sequence_length = input_values.shape\n",
    "sequence_length = model._get_feat_extract_output_lengths(raw_sequence_length).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_time_indices = _compute_mask_indices(\n",
    "    shape=(batch_size, sequence_length), mask_prob=0.2, mask_length=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_negative_indices = _sample_negative_indices(\n",
    "    features_shape=(batch_size, sequence_length),\n",
    "    num_negatives=model.config.num_negatives,\n",
    "    mask_time_indices=mask_time_indices,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_time_indices = torch.tensor(\n",
    "    data=mask_time_indices, device=input_values.device, dtype=torch.long\n",
    ")\n",
    "sampled_negative_indices = torch.tensor(\n",
    "    data=sampled_negative_indices, device=input_values.device, dtype=torch.long\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_negative_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(input_values, mask_time_indices=mask_time_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = torch.cosine_similarity(\n",
    "    outputs.projected_states, outputs.projected_quantized_states, dim=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim[mask_time_indices.to(torch.bool)].mean() > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.train()\n",
    "loss = model(\n",
    "    input_values,\n",
    "    mask_time_indices=mask_time_indices,\n",
    "    sampled_negative_indices=sampled_negative_indices,\n",
    ").loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(276.4055, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(\n",
    "    input_values,\n",
    "    mask_time_indices=mask_time_indices,\n",
    "    sampled_negative_indices=sampled_negative_indices,\n",
    ").loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
