{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from utils import *\n",
    "from feature_encoder import ConvFeatureExtractionModel\n",
    "from gumbel import GumbelVectorQuantizer\n",
    "import math\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 220500])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = []\n",
    "for i in range(1,3):\n",
    "    path = f\"data/mp3_train_files/Gould/Gould - WTC_clip_{i}.mp3\"\n",
    "    waveform,sample_rate = torchaudio.load(path)\n",
    "    waveform = torch.mean(waveform, dim=0).unsqueeze(0)\n",
    "    waveform = normalize_tensor(waveform)\n",
    "    x.append(waveform)\n",
    "\n",
    "\n",
    "x = torch.cat(x)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_waveform(x, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "receptive_field=1328 samples\n",
      "30.113 ms\n"
     ]
    }
   ],
   "source": [
    "dim = 512 # quadratic to number of params in conv... double dim quadruple params\n",
    "\n",
    "conv_feature_layers = (\n",
    "    [(dim, 10, 2)] * 3 + [(dim, 3, 2)] * 5 + [(dim, 2, 2)] + [(dim, 2, 2)]\n",
    ")  # (dim,kernel width, strides) # kernel_width is window, string is len of slide\n",
    "#conv_feature_layers = [(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512, 2, 2)] + [(512, 2, 2)]\n",
    "receptive_field = calculate_receptive_field(conv_feature_layers)\n",
    "print(f\"{receptive_field=} samples\")\n",
    "print(f\"{round(receptive_field / (sample_rate / 1000),3)} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num params:  10238976\n",
      "input shape= torch.Size([2, 220500])\n",
      "torch.Size([2, 1, 220500])\n",
      "torch.Size([2, 512, 110246])\n",
      "torch.Size([2, 512, 55119])\n",
      "torch.Size([2, 512, 27555])\n",
      "torch.Size([2, 512, 13777])\n",
      "torch.Size([2, 512, 6888])\n",
      "torch.Size([2, 512, 3443])\n",
      "torch.Size([2, 512, 1721])\n",
      "torch.Size([2, 512, 860])\n",
      "torch.Size([2, 512, 430])\n",
      "torch.Size([2, 512, 215])\n",
      "output shape=  torch.Size([2, 512, 215])\n"
     ]
    }
   ],
   "source": [
    "conv = ConvFeatureExtractionModel(conv_feature_layers, mode=\"layer_norm\")\n",
    "print('num params: ', conv.params())\n",
    "print('input shape=',x.shape)\n",
    "conv_out = conv(x, verbose=True)\n",
    "print(\"output shape= \", conv_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512, 215])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class GumbelVectorQuantizer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_vars,\n",
    "        temp,\n",
    "        groups,\n",
    "        combine_groups,\n",
    "        vq_dim,\n",
    "        time_first,\n",
    "        activation=nn.GELU(),\n",
    "        weight_proj_depth=1,\n",
    "        weight_proj_factor=1,\n",
    "    ):\n",
    "        \"\"\"Vector quantization using gumbel softmax\n",
    "\n",
    "        Args:\n",
    "            dim: input dimension (channels)\n",
    "            num_vars: number of quantized vectors per group\n",
    "            temp: temperature for training. this should be a tuple of 3 elements: (start, stop, decay factor)\n",
    "            groups: number of groups for vector quantization\n",
    "            combine_groups: whether to use the vectors for all groups,\n",
    "                            concat the code book to make 1 larger codebook, for experimenting with config\n",
    "            vq_dim: dimensionality of the resulting quantized vector\n",
    "            time_first: if true, expect input in BxTxC format, otherwise in BxCxT\n",
    "            activation: what activation to use (should be a module). this is only used if weight_proj_depth is > 1\n",
    "            weight_proj_depth: number of layers (with activation in between) to project input before computing logits\n",
    "            weight_proj_factor: this is used only if weight_proj_depth is > 1. scales the inner dimensionality of\n",
    "                                projections by this factor\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.groups = groups\n",
    "        self.combine_groups = combine_groups\n",
    "        self.input_dim = dim\n",
    "        self.num_vars = num_vars\n",
    "        self.time_first = time_first\n",
    "\n",
    "        assert (\n",
    "            vq_dim % groups == 0\n",
    "        ), f\"dim {vq_dim} must be divisible by groups {groups} for concatenation\"\n",
    "\n",
    "        var_dim = vq_dim // groups\n",
    "        num_groups = groups if not combine_groups else 1\n",
    "\n",
    "        self.vars = nn.Parameter(torch.FloatTensor(1, num_groups * num_vars, var_dim))\n",
    "        nn.init.uniform_(self.vars)\n",
    "\n",
    "        if weight_proj_depth > 1:\n",
    "\n",
    "            def block(input_dim, output_dim):\n",
    "                return nn.Sequential(nn.Linear(input_dim, output_dim), activation)\n",
    "\n",
    "            inner_dim = self.input_dim * weight_proj_factor\n",
    "            self.weight_proj = nn.Sequential(\n",
    "                *[\n",
    "                    block(self.input_dim if i == 0 else inner_dim, inner_dim)\n",
    "                    for i in range(weight_proj_depth - 1)\n",
    "                ],\n",
    "                nn.Linear(inner_dim, groups * num_vars),\n",
    "            )\n",
    "        else:\n",
    "            self.weight_proj = nn.Linear(self.input_dim, groups * num_vars)\n",
    "            nn.init.normal_(self.weight_proj.weight, mean=0, std=1)\n",
    "            nn.init.zeros_(self.weight_proj.bias)\n",
    "\n",
    "        if isinstance(temp, str):\n",
    "            import ast\n",
    "\n",
    "            temp = ast.literal_eval(temp)\n",
    "        assert len(temp) == 3, f\"{temp}, {len(temp)}\"\n",
    "\n",
    "        self.max_temp, self.min_temp, self.temp_decay = temp\n",
    "        self.curr_temp = self.max_temp\n",
    "        self.codebook_indices = None\n",
    "\n",
    "    def set_num_updates(self, num_updates):\n",
    "        self.curr_temp = max(\n",
    "            self.max_temp * self.temp_decay**num_updates, self.min_temp\n",
    "        )\n",
    "\n",
    "    def get_codebook_indices(self):\n",
    "        if self.codebook_indices is None:\n",
    "            from itertools import product\n",
    "\n",
    "            p = [range(self.num_vars)] * self.groups\n",
    "            inds = list(product(*p))\n",
    "            self.codebook_indices = torch.tensor(\n",
    "                inds, dtype=torch.long, device=self.vars.device\n",
    "            ).flatten()\n",
    "\n",
    "            if not self.combine_groups:\n",
    "                self.codebook_indices = self.codebook_indices.view(\n",
    "                    self.num_vars**self.groups, -1\n",
    "                )\n",
    "                for b in range(1, self.groups):\n",
    "                    self.codebook_indices[:, b] += self.num_vars * b\n",
    "                self.codebook_indices = self.codebook_indices.flatten()\n",
    "        return self.codebook_indices\n",
    "\n",
    "    def codebook(self):\n",
    "        indices = self.get_codebook_indices()\n",
    "        return (\n",
    "            self.vars.squeeze(0)\n",
    "            .index_select(0, indices)\n",
    "            .view(self.num_vars**self.groups, -1)\n",
    "        )\n",
    "\n",
    "    def sample_from_codebook(self, b, n):\n",
    "        indices = self.get_codebook_indices()\n",
    "        indices = indices.view(-1, self.groups)\n",
    "        cb_size = indices.size(0)\n",
    "        assert (\n",
    "            n < cb_size\n",
    "        ), f\"sample size {n} is greater than size of codebook {cb_size}\"\n",
    "        sample_idx = torch.randint(low=0, high=cb_size, size=(b * n,))\n",
    "        indices = indices[sample_idx]\n",
    "\n",
    "        z = self.vars.squeeze(0).index_select(0, indices.flatten()).view(b, n, -1)\n",
    "        return z\n",
    "\n",
    "    def to_codebook_index(self, indices):\n",
    "        res = indices.new_full(indices.shape[:-1], 0)\n",
    "        for i in range(self.groups):\n",
    "            exponent = self.groups - i - 1\n",
    "            res += indices[..., i] * (self.num_vars**exponent)\n",
    "        return res\n",
    "\n",
    "    def forward_idx(self, x):\n",
    "        res = self.forward(x, produce_targets=True)\n",
    "        return res[\"x\"], res[\"targets\"]\n",
    "\n",
    "    def forward(self, x, produce_targets=False):\n",
    "        print(x.shape)\n",
    "\n",
    "        result = {\"num_vars\": self.num_vars * self.groups}\n",
    "\n",
    "        if not self.time_first:\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "        bsz, tsz, fsz = x.shape\n",
    "        x = x.reshape(-1, fsz)\n",
    "        x = self.weight_proj(x)\n",
    "        x = x.view(bsz * tsz * self.groups, -1)\n",
    "\n",
    "        _, k = x.max(-1)\n",
    "        hard_x = (\n",
    "            x.new_zeros(*x.shape)\n",
    "            .scatter_(-1, k.view(-1, 1), 1.0)\n",
    "            .view(bsz * tsz, self.groups, -1)\n",
    "        )\n",
    "        hard_probs = torch.mean(hard_x.float(), dim=0)\n",
    "        result[\"code_perplexity\"] = torch.exp(\n",
    "            -torch.sum(hard_probs * torch.log(hard_probs + 1e-7), dim=-1)\n",
    "        ).sum()\n",
    "\n",
    "        avg_probs = torch.softmax(\n",
    "            x.view(bsz * tsz, self.groups, -1).float(), dim=-1\n",
    "        ).mean(dim=0)\n",
    "        result[\"prob_perplexity\"] = torch.exp(\n",
    "            -torch.sum(avg_probs * torch.log(avg_probs + 1e-7), dim=-1)\n",
    "        ).sum()\n",
    "\n",
    "        result[\"temp\"] = self.curr_temp\n",
    "\n",
    "        if self.training:\n",
    "            x = F.gumbel_softmax(x.float(), tau=self.curr_temp, hard=True).type_as(x)\n",
    "        else:\n",
    "            x = hard_x\n",
    "\n",
    "        x = x.view(bsz * tsz, -1)\n",
    "\n",
    "        vars = self.vars\n",
    "        if self.combine_groups:\n",
    "            vars = vars.repeat(1, self.groups, 1)\n",
    "\n",
    "        if produce_targets:\n",
    "            result[\"targets\"] = (\n",
    "                x.view(bsz * tsz * self.groups, -1)\n",
    "                .argmax(dim=-1)\n",
    "                .view(bsz, tsz, self.groups)\n",
    "                .detach()\n",
    "            )\n",
    "\n",
    "        x = x.unsqueeze(-1) * vars\n",
    "        x = x.view(bsz * tsz, self.groups, self.num_vars, -1)\n",
    "        x = x.sum(-2)\n",
    "        x = x.view(bsz, tsz, -1)\n",
    "\n",
    "        if not self.time_first:\n",
    "            x = x.transpose(1, 2)  # BTC -> BCT\n",
    "\n",
    "        result[\"x\"] = x\n",
    "\n",
    "        print(x.shape)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_vars = 320\n",
    "temp_default = (2, 0.5, 0.999995)\n",
    "groups = 2\n",
    "vq_dim = 768 # output_dim? to match input of attn\n",
    "weight_proj_factor = 3\n",
    "gum = GumbelVectorQuantizer(\n",
    "    dim=dim,\n",
    "    num_vars=latent_vars,\n",
    "    temp=temp_default,\n",
    "    combine_groups=False,\n",
    "    groups=groups,\n",
    "    vq_dim=vq_dim,\n",
    "    time_first=False,\n",
    "    weight_proj_depth=1,\n",
    "    weight_proj_factor=weight_proj_factor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
