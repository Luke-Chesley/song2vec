{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoFeatureExtractor, Wav2Vec2ForPreTraining,get_scheduler\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import _compute_mask_indices, _sample_negative_indices\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from model import Wav2Vec2ForPreTraining,Wav2Vec2Config\n",
    "from dataset import AudioDatasetSplits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/projects/jupyterlab/Notebooks/song2vec/venv/lib/python3.10/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "config = Wav2Vec2Config()\n",
    "model = Wav2Vec2ForPreTraining(config)\n",
    "ds_train = AudioDatasetSplits().train\n",
    "ds_val = AudioDatasetSplits().val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ips = feature_extractor(ds_train[0]['audio'],return_tensors='pt',sampling_rate=ds_train[0]['sample_rate']).input_values\n",
    "batch_size, raw_sequence_length = ips.shape\n",
    "sequence_length = model._get_feat_extract_output_lengths(raw_sequence_length).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_time_indices = _compute_mask_indices(\n",
    "    shape=(batch_size, sequence_length), mask_prob=model.config.mask_time_prob, mask_length=model.config.mask_time_length\n",
    ")\n",
    "\n",
    "sampled_negative_indices = _sample_negative_indices(\n",
    "    features_shape=(batch_size, sequence_length),\n",
    "    num_negatives=model.config.num_negatives,\n",
    "    mask_time_indices=mask_time_indices,\n",
    ")\n",
    "\n",
    "mask_time_indices = torch.tensor(data=mask_time_indices, device=ips.device, dtype=torch.long)\n",
    "sampled_negative_indices = torch.tensor(\n",
    "    data=sampled_negative_indices, device=ips.device, dtype=torch.long\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.train()\n",
    "loss = model(\n",
    "    ips, mask_time_indices=mask_time_indices, sampled_negative_indices=sampled_negative_indices\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForPreTrainingOutput(loss=tensor(nan, grad_fn=<AddBackward0>), projected_states=tensor([[[-0.2438,  0.5366, -0.6080,  ..., -0.7227, -0.4334,  0.8144],\n",
       "         [-0.5922, -0.4866, -0.0443,  ..., -0.0500, -0.0972,  0.4619],\n",
       "         [ 0.0490,  0.1360,  0.2218,  ..., -0.3461,  0.3827,  0.0350],\n",
       "         ...,\n",
       "         [-0.7289, -0.2255,  0.3768,  ..., -0.6308, -0.2128,  0.6229],\n",
       "         [ 0.5319,  0.2309, -0.1708,  ..., -0.1450,  0.0904, -0.5288],\n",
       "         [ 0.4541, -0.2181,  0.1276,  ...,  0.1025,  0.1758,  0.0971]]],\n",
       "       grad_fn=<ViewBackward0>), projected_quantized_states=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]]], grad_fn=<ViewBackward0>), codevector_perplexity=tensor(613.7838, grad_fn=<SumBackward0>), hidden_states=None, attentions=None, contrastive_loss=tensor(nan, grad_fn=<NllLossBackward0>), diversity_loss=tensor(0.8193, grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(ds_train,batch_size=8) # can put feature extractor in ds\n",
    "dl_val = DataLoader(ds_val,batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = AdamW(\n",
    "#         list(model.parameters()),\n",
    "#         lr=5e-4,\n",
    "#         betas=[args.adam_beta1, args.adam_beta2],\n",
    "#         eps=args.adam_epsilon,\n",
    "#     )\n",
    "\n",
    "\n",
    "# lr_scheduler = get_scheduler(\n",
    "#         name=args.lr_scheduler_type,\n",
    "#         optimizer=optimizer,\n",
    "#         num_warmup_steps=args.num_warmup_steps,\n",
    "#         num_training_steps=args.max_train_steps,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 00:   0%|          | 0/1550 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.2682e-04,  2.3256e-04,  2.2961e-04,  ...,  1.5293e+00,\n",
      "          1.4560e+00,  1.5053e+00],\n",
      "        [ 1.5130e-01,  2.7549e-01,  2.4681e-01,  ...,  3.8436e-01,\n",
      "          5.6157e-01,  7.9311e-01],\n",
      "        [-3.5496e-04, -3.5496e-04, -3.5496e-04,  ..., -5.8199e-01,\n",
      "         -6.8625e-01, -7.9274e-01],\n",
      "        ...,\n",
      "        [ 9.8245e-04,  9.8096e-04,  9.8080e-04,  ..., -7.8956e-01,\n",
      "         -1.1840e+00, -1.6731e+00],\n",
      "        [-5.1873e-04, -5.1843e-04, -5.1778e-04,  ...,  1.8853e+00,\n",
      "          1.5824e+00,  1.3594e+00],\n",
      "        [ 2.7939e-04,  2.7939e-04,  2.7939e-04,  ...,  1.0347e-01,\n",
      "          1.0546e-01,  1.1822e-01]])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "tensor([[[   0,    0,    0,  ...,    0,    0,    0],\n",
      "         [   0,    0,    0,  ...,    0,    0,    0],\n",
      "         [   0,    0,    0,  ...,    0,    0,    0],\n",
      "         ...,\n",
      "         [   0,    0,    0,  ...,    0,    0,    0],\n",
      "         [   0,    0,    0,  ...,    0,    0,    0],\n",
      "         [   0,    0,    0,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 249,  249,  249,  ...,  249,  249,  249],\n",
      "         [ 249,  249,  249,  ...,  249,  249,  249],\n",
      "         [ 249,  249,  249,  ...,  249,  249,  249],\n",
      "         ...,\n",
      "         [ 266,  269,  263,  ...,  260,  268,  261],\n",
      "         [ 249,  249,  249,  ...,  249,  249,  249],\n",
      "         [ 249,  249,  249,  ...,  249,  249,  249]],\n",
      "\n",
      "        [[ 498,  498,  498,  ...,  498,  498,  498],\n",
      "         [ 498,  498,  498,  ...,  498,  498,  498],\n",
      "         [ 498,  498,  498,  ...,  498,  498,  498],\n",
      "         ...,\n",
      "         [ 498,  498,  498,  ...,  498,  498,  498],\n",
      "         [ 498,  498,  498,  ...,  498,  498,  498],\n",
      "         [ 498,  498,  498,  ...,  498,  498,  498]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1245, 1245, 1245,  ..., 1245, 1245, 1245],\n",
      "         [1245, 1245, 1245,  ..., 1245, 1245, 1245],\n",
      "         [1245, 1245, 1245,  ..., 1245, 1245, 1245],\n",
      "         ...,\n",
      "         [1245, 1245, 1245,  ..., 1245, 1245, 1245],\n",
      "         [1245, 1245, 1245,  ..., 1245, 1245, 1245],\n",
      "         [1245, 1245, 1245,  ..., 1245, 1245, 1245]],\n",
      "\n",
      "        [[1494, 1494, 1494,  ..., 1494, 1494, 1494],\n",
      "         [1494, 1494, 1494,  ..., 1494, 1494, 1494],\n",
      "         [1494, 1494, 1494,  ..., 1494, 1494, 1494],\n",
      "         ...,\n",
      "         [1494, 1494, 1494,  ..., 1494, 1494, 1494],\n",
      "         [1494, 1494, 1494,  ..., 1494, 1494, 1494],\n",
      "         [1494, 1494, 1494,  ..., 1494, 1494, 1494]],\n",
      "\n",
      "        [[1743, 1743, 1743,  ..., 1743, 1743, 1743],\n",
      "         [1743, 1743, 1743,  ..., 1743, 1743, 1743],\n",
      "         [1743, 1743, 1743,  ..., 1743, 1743, 1743],\n",
      "         ...,\n",
      "         [1743, 1743, 1743,  ..., 1743, 1743, 1743],\n",
      "         [1743, 1743, 1743,  ..., 1743, 1743, 1743],\n",
      "         [1743, 1743, 1743,  ..., 1743, 1743, 1743]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 00:   0%|          | 0/1550 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(764.9269, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_epochs = 0\n",
    "total_epochs = 1\n",
    "\n",
    "for epoch in range(start_epochs,total_epochs):\n",
    "    model.train()\n",
    "    batch_iterator = tqdm(dl_train,desc=f\"Processing Epoch {epoch:02d}\")\n",
    "    for batch_idx,batch in enumerate(batch_iterator):\n",
    "\n",
    "        #batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "\n",
    "\n",
    "        \n",
    "        ips = feature_extractor(batch['audio'].numpy(),sampling_rate=batch['sample_rate'][0],return_tensors='pt').input_values\n",
    "\n",
    "        batch_size,raw_sequence_length = ips.shape\n",
    "\n",
    "        sequence_length = model._get_feat_extract_output_lengths(raw_sequence_length).item()\n",
    "\n",
    "\n",
    "        mask_time_indices = _compute_mask_indices(\n",
    "            shape=(batch_size, sequence_length), mask_prob=model.config.mask_time_prob, mask_length=model.config.mask_time_length\n",
    "        )\n",
    "\n",
    "        sampled_negative_indices = _sample_negative_indices(\n",
    "            features_shape=(batch_size, sequence_length),\n",
    "            num_negatives=model.config.num_negatives,\n",
    "            mask_time_indices=mask_time_indices,\n",
    "        )\n",
    "\n",
    "        mask_time_indices = torch.tensor(data=mask_time_indices, device=ips.device, dtype=torch.long)\n",
    "        sampled_negative_indices = torch.tensor(\n",
    "            data=sampled_negative_indices, device=ips.device, dtype=torch.long\n",
    "        )\n",
    "\n",
    "        print(ips)\n",
    "        print(mask_time_indices)\n",
    "        print(sampled_negative_indices)\n",
    "\n",
    "        loss = model(ips,mask_time_indices=mask_time_indices,sampled_negative_indices=sampled_negative_indices).loss\n",
    "\n",
    "        print('loss',loss)\n",
    "\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        batch_iterator.set_postfix(loss=running_loss /( 1 if batch_idx == 0 else batch_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
