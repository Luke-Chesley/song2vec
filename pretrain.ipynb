{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Union\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from datasets import DatasetDict, concatenate_datasets, load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    "    is_wandb_available,\n",
    "    set_seed,\n",
    "    Wav2Vec2FeatureExtractor\n",
    ")\n",
    "from model import (\n",
    "    Wav2Vec2ForPreTraining,\n",
    "    Wav2Vec2Config,\n",
    "    Wav2Vec2FeatureEncoder\n",
    ")\n",
    "\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import _compute_mask_indices, _sample_negative_indices\n",
    "from transformers.utils import send_example_telemetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForWav2Vec2Pretraining:\n",
    "\n",
    "\n",
    "    model: Wav2Vec2ForPreTraining\n",
    "    feature_extractor: Wav2Vec2FeatureExtractor\n",
    "    padding: Union[bool, str] = \"longest\"\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    mask_time_prob: Optional[float] = 0.65\n",
    "    mask_time_length: Optional[int] = 10\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # reformat list to dict and set to pytorch format\n",
    "\n",
    "        input_values = [feature['input_values'][0] for feature in features]  # Note the [0] to get the tensor from the list\n",
    "\n",
    "        # Wrap input_values in a dictionary\n",
    "        inputs_dict = {'input_values': input_values}\n",
    "\n",
    "\n",
    "        batch = self.feature_extractor.pad(\n",
    "            inputs_dict,\n",
    "            padding=self.padding,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        device = batch[\"input_values\"].device\n",
    "        batch_size = batch[\"input_values\"].shape[0]\n",
    "\n",
    "        mask_indices_seq_length = self.model._get_feat_extract_output_lengths(batch[\"input_values\"].shape[-1])\n",
    "        # make sure masked sequence length is a Python scalar\n",
    "        mask_indices_seq_length = int(mask_indices_seq_length)\n",
    "\n",
    "        # make sure that no loss is computed on padded inputs\n",
    "        if batch.get(\"attention_mask\") is not None:\n",
    "            # compute real output lengths according to convolution formula\n",
    "            batch[\"sub_attention_mask\"] = self.model._get_feature_vector_attention_mask(\n",
    "                mask_indices_seq_length, batch[\"attention_mask\"]\n",
    "            )\n",
    "\n",
    "        features_shape = (batch_size, mask_indices_seq_length)\n",
    "\n",
    "        # sample randomly masked indices\n",
    "        mask_time_indices = _compute_mask_indices(\n",
    "            features_shape,\n",
    "            self.mask_time_prob,\n",
    "            self.mask_time_length,\n",
    "            attention_mask=batch.get(\"sub_attention_mask\"),\n",
    "        )\n",
    "\n",
    "        # sample negative indices\n",
    "        sampled_negative_indices = _sample_negative_indices(\n",
    "            features_shape,\n",
    "            self.model.config.num_negatives,\n",
    "            mask_time_indices=mask_time_indices,\n",
    "        )\n",
    "        batch[\"mask_time_indices\"] = torch.tensor(mask_time_indices, dtype=torch.long, device=device)\n",
    "        batch[\"sampled_negative_indices\"] = torch.tensor(sampled_negative_indices, dtype=torch.long, device=device)\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "def multiply_grads(params, c):\n",
    "    \"\"\"Multiplies grads by a constant *c*.\"\"\"\n",
    "    for p in params:\n",
    "        if p.grad is not None:\n",
    "            if torch.is_tensor(c):\n",
    "                c = c.to(p.grad.device)\n",
    "            p.grad.data.mul_(c)\n",
    "\n",
    "\n",
    "def get_grad_norm(params, scale=1):\n",
    "    \"\"\"Compute grad norm given a gradient scale.\"\"\"\n",
    "    total_norm = 0.0\n",
    "    for p in params:\n",
    "        if p.grad is not False:\n",
    "            print('not none')\n",
    "            print(p)\n",
    "            param_norm = (p.grad.detach().data / scale).norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm**0.5\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import AudioDataset\n",
    "import random\n",
    "\n",
    "parent_dir = 'data/mp3_train_files'\n",
    "file_list = [os.path.join(root, file) \n",
    "             for root, _, files in os.walk(parent_dir) \n",
    "             for file in files]\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(file_list)\n",
    "\n",
    "train_size = int(0.8 * len(file_list))\n",
    "val_size = int(0.1 * len(file_list))\n",
    "test_size = len(file_list) - train_size - val_size\n",
    "\n",
    "train_files = file_list[:train_size]\n",
    "val_files = file_list[train_size:train_size + val_size]\n",
    "test_files = file_list[train_size + val_size:]\n",
    "\n",
    "train_dataset = AudioDataset(train_files)\n",
    "val_dataset = AudioDataset(val_files)\n",
    "test_dataset = AudioDataset(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Wav2Vec2Config()\n",
    "feature_extractor = Wav2Vec2FeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Wav2Vec2ForPreTraining(config).to(device)\n",
    "\n",
    "mask_time_prob = config.mask_time_prob\n",
    "mask_time_length = config.mask_time_length \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print('true' if param.requires_grad else 'false')\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForWav2Vec2Pretraining(\n",
    "        model=model,\n",
    "        feature_extractor=feature_extractor,\n",
    "        #pad_to_multiple_of=args.pad_to_multiple_of,\n",
    "        mask_time_prob=mask_time_prob,\n",
    "        mask_time_length=mask_time_length,\n",
    "    )\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator,\n",
    "        batch_size=8,\n",
    "    )\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "        val_dataset, collate_fn=data_collator, batch_size=8\n",
    "    )\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=0.004, # 5e-5\n",
    "        betas=[0.9, 0.999],\n",
    "        eps=1e-6,\n",
    "    )\n",
    "\n",
    "# model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "#         model, optimizer, train_dataloader, eval_dataloader\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / 1)\n",
    "max_train_steps = 3 * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "        name='linear',\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=10,\n",
    "        num_training_steps=max_train_steps,\n",
    "    )\n",
    "\n",
    "num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_batch_size = 4 \n",
    "max_gumbel_temperature = 2.0\n",
    "min_gumbel_temperature = 0.5\n",
    "gumbel_temperature_decay = 0.999995\n",
    "logging_steps = 10\n",
    "gradient_accumulation_steps = 8\n",
    "saving_steps = 500\n",
    "push_to_hub = False\n",
    "output_dir = 'weights'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "log_dir = \"runs/2\"  # Change this to your desired log directory\n",
    "writer = SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Values: tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0910e-01,\n",
      "         -9.8474e-02, -1.1041e-01],\n",
      "        [ 8.9839e-10,  1.6265e-09,  1.7210e-09,  ..., -9.8750e-02,\n",
      "         -7.6209e-02, -1.3102e-01],\n",
      "        [ 1.0619e-01,  2.2158e-01,  2.0276e-01,  ...,  6.2053e-01,\n",
      "          6.5487e-01,  8.0091e-01],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  2.4637e-01,\n",
      "          2.3559e-01,  2.1247e-01],\n",
      "        [ 6.6088e-11,  1.2577e-10,  9.7566e-11,  ..., -3.2955e-02,\n",
      "         -4.2624e-02, -6.0856e-02],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  8.2496e-02,\n",
      "          8.7817e-02,  1.0291e-01]])\n",
      "Min: -1.0, Max: 1.0\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "input_values = batch['input_values']\n",
    "print(\"Input Values:\", input_values)\n",
    "\n",
    "# Check for extreme values\n",
    "print(f\"Min: {input_values.min()}, Max: {input_values.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinitializing wav2vec2.feature_extractor.conv_layers.0.conv.weight\n",
      "Reinitializing wav2vec2.feature_extractor.conv_layers.0.layer_norm.weight\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Fan in and fan out can not be computed for tensor with fewer than 2 dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReinitializing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name:\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxavier_normal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name:\n\u001b[1;32m      8\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mzeros_(param)\n",
      "File \u001b[0;32m~/projects/jupyterlab/Notebooks/song2vec/venv/lib/python3.10/site-packages/torch/nn/init.py:390\u001b[0m, in \u001b[0;36mxavier_normal_\u001b[0;34m(tensor, gain, generator)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mxavier_normal_\u001b[39m(\n\u001b[1;32m    366\u001b[0m     tensor: Tensor,\n\u001b[1;32m    367\u001b[0m     gain: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m    368\u001b[0m     generator: _Optional[torch\u001b[38;5;241m.\u001b[39mGenerator] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    369\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    370\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Fill the input `Tensor` with values using a Xavier normal distribution.\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m    The method is described in `Understanding the difficulty of training deep feedforward\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;124;03m        >>> nn.init.xavier_normal_(w)\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 390\u001b[0m     fan_in, fan_out \u001b[38;5;241m=\u001b[39m \u001b[43m_calculate_fan_in_and_fan_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m     std \u001b[38;5;241m=\u001b[39m gain \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(fan_in \u001b[38;5;241m+\u001b[39m fan_out))\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _no_grad_normal_(tensor, \u001b[38;5;241m0.\u001b[39m, std, generator)\n",
      "File \u001b[0;32m~/projects/jupyterlab/Notebooks/song2vec/venv/lib/python3.10/site-packages/torch/nn/init.py:318\u001b[0m, in \u001b[0;36m_calculate_fan_in_and_fan_out\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m    316\u001b[0m dimensions \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mdim()\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dimensions \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFan in and fan out can not be computed for tensor with fewer than 2 dimensions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    320\u001b[0m num_input_fmaps \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    321\u001b[0m num_output_fmaps \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Fan in and fan out can not be computed for tensor with fewer than 2 dimensions"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2ForPreTraining(config)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
    "progress_bar = tqdm(range(max_train_steps))\n",
    "completed_steps = 0\n",
    "starting_epoch = 0\n",
    "\n",
    "for epoch in range(starting_epoch, num_train_epochs):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        if torch.isnan(loss).any() or torch.isinf(loss).any():\n",
    "            print(\"NaN or Inf detected in loss\")\n",
    "            continue\n",
    "\n",
    "        print(\"Loss value:\", loss.item())\n",
    "        print(\"Requires grad:\", loss.requires_grad)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        grads = [p.grad for p in model.parameters() if p.grad is not None]\n",
    "        if not grads:\n",
    "            print(\"No gradients computed\")\n",
    "            continue\n",
    "        else:\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    print(f\"{name}: grad norm = {param.grad.norm().item()}\")\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        completed_steps += 1\n",
    "        progress_bar.update(1)\n",
    "\n",
    "        if completed_steps >= max_train_steps:\n",
    "            break\n",
    "\n",
    "    # Validation code here (omitted for brevity)\n",
    "\n",
    "progress_bar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d792c20eaec4a73a9a04ac79bc221a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4650 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not none\n",
      "Parameter containing:\n",
      "tensor([8.4824e-02, 9.8099e-01, 4.5328e-01, 4.3967e-01, 5.0145e-01, 1.8659e-01,\n",
      "        2.7512e-01, 2.2131e-02, 1.4914e-01, 1.1665e-01, 2.8491e-01, 4.4491e-01,\n",
      "        9.8502e-01, 3.4413e-01, 2.9262e-01, 1.1975e-01, 4.6609e-01, 1.9719e-01,\n",
      "        4.1189e-01, 3.7040e-01, 5.7756e-01, 3.1124e-01, 8.7557e-01, 7.7711e-02,\n",
      "        3.9245e-01, 2.3488e-01, 8.5205e-01, 8.7894e-01, 3.0212e-01, 7.3275e-02,\n",
      "        7.8919e-01, 6.7912e-01, 2.2240e-01, 2.6755e-01, 5.2048e-01, 1.6625e-01,\n",
      "        7.6600e-01, 7.9738e-01, 2.7697e-01, 2.3656e-01, 7.1059e-01, 1.8312e-01,\n",
      "        5.6537e-01, 7.8787e-01, 6.5091e-01, 4.5836e-01, 6.0524e-01, 8.9491e-02,\n",
      "        2.2072e-01, 3.9741e-01, 6.5188e-01, 4.2748e-01, 3.5178e-01, 2.7553e-01,\n",
      "        9.9817e-01, 5.9067e-01, 1.4937e-02, 9.8916e-01, 9.2044e-01, 5.5102e-01,\n",
      "        8.6186e-01, 4.1171e-01, 5.8003e-01, 3.1871e-01, 6.7465e-02, 7.6799e-01,\n",
      "        9.1963e-01, 9.1382e-01, 2.5272e-02, 2.3235e-01, 7.9589e-01, 1.1008e-01,\n",
      "        9.3207e-01, 9.0076e-01, 9.9762e-01, 7.8295e-01, 7.8196e-01, 3.4258e-01,\n",
      "        6.5594e-01, 5.6514e-01, 1.5807e-01, 6.7778e-01, 4.5055e-01, 7.8271e-01,\n",
      "        9.5349e-01, 6.7738e-01, 7.6623e-02, 4.1282e-01, 3.5178e-01, 9.1333e-01,\n",
      "        6.1644e-01, 9.0903e-01, 7.2996e-01, 9.7282e-01, 9.8811e-01, 8.1825e-01,\n",
      "        1.5614e-01, 4.5992e-01, 2.8517e-03, 7.5282e-01, 6.7197e-01, 3.6828e-01,\n",
      "        6.0925e-02, 8.2621e-01, 2.7115e-01, 5.1838e-01, 8.5919e-01, 5.0917e-03,\n",
      "        4.9071e-01, 3.5263e-01, 5.2175e-01, 3.2155e-01, 7.7749e-01, 7.8121e-01,\n",
      "        4.6464e-01, 9.0407e-01, 2.6732e-02, 8.2510e-01, 9.7426e-01, 2.1628e-02,\n",
      "        1.1867e-01, 3.4969e-01, 6.1594e-01, 2.1138e-01, 5.2954e-02, 6.3628e-01,\n",
      "        8.8924e-01, 8.1124e-02, 2.8572e-01, 2.9905e-01, 1.9385e-01, 7.7757e-01,\n",
      "        7.6841e-01, 5.8828e-01, 1.4606e-01, 9.0400e-01, 6.6971e-01, 4.2185e-01,\n",
      "        9.3127e-01, 6.1895e-01, 1.9815e-01, 4.3115e-01, 7.9264e-01, 4.8293e-02,\n",
      "        3.2750e-01, 5.9981e-01, 4.5029e-02, 8.4083e-01, 7.1817e-02, 3.5725e-01,\n",
      "        5.7307e-01, 5.1349e-02, 2.0241e-01, 3.4125e-01, 3.6925e-02, 6.0905e-01,\n",
      "        5.7269e-01, 6.2385e-01, 7.2230e-01, 6.8858e-01, 3.6462e-01, 5.5074e-02,\n",
      "        8.1310e-01, 6.9931e-01, 1.0261e-01, 6.1047e-01, 8.0528e-01, 7.3517e-01,\n",
      "        5.6925e-01, 6.2792e-01, 2.8206e-01, 1.1562e-01, 6.4785e-02, 1.9619e-01,\n",
      "        5.2544e-02, 7.2354e-02, 3.5719e-01, 2.6656e-01, 4.5285e-01, 5.8557e-01,\n",
      "        7.3742e-01, 7.3713e-01, 8.2214e-01, 1.8275e-01, 8.9718e-01, 1.2518e-01,\n",
      "        1.4717e-02, 6.1257e-01, 4.7074e-01, 5.5295e-01, 6.1132e-01, 9.2406e-01,\n",
      "        7.0617e-01, 7.0760e-01, 8.8681e-01, 1.0890e-01, 8.6615e-01, 4.0680e-01,\n",
      "        4.9083e-01, 1.5538e-01, 8.6443e-01, 2.3995e-01, 8.7223e-01, 7.0661e-01,\n",
      "        9.7449e-01, 4.7516e-01, 5.1597e-01, 1.6355e-01, 2.1695e-01, 3.8407e-01,\n",
      "        7.3833e-01, 1.0650e-02, 1.7912e-01, 5.4048e-01, 2.0273e-01, 7.7593e-02,\n",
      "        9.3677e-01, 9.0170e-01, 5.7183e-01, 7.8415e-01, 3.4343e-01, 7.4040e-01,\n",
      "        5.0922e-01, 2.7589e-01, 3.4505e-01, 9.9278e-01, 1.7058e-01, 4.7474e-01,\n",
      "        6.0744e-01, 3.5933e-01, 1.0362e-02, 1.0726e-01, 6.3330e-01, 3.0575e-01,\n",
      "        9.2735e-01, 7.6550e-01, 4.5483e-01, 5.6050e-02, 6.8423e-01, 4.0734e-01,\n",
      "        7.5402e-01, 8.6286e-01, 1.7934e-01, 8.6081e-01, 6.3106e-01, 7.1914e-01,\n",
      "        3.7378e-03, 3.7118e-01, 2.9971e-01, 3.9911e-01, 1.3674e-01, 3.6431e-01,\n",
      "        7.1475e-01, 7.5816e-01, 4.2078e-01, 6.2534e-01, 1.7065e-01, 8.2430e-01,\n",
      "        7.5787e-01, 2.0550e-01, 6.8330e-01, 3.4750e-01, 3.1454e-01, 1.8342e-01,\n",
      "        4.0272e-01, 8.9292e-01, 9.8934e-01, 6.2846e-01, 6.2540e-01, 3.4773e-01,\n",
      "        3.0719e-01, 4.0468e-01, 2.6591e-01, 9.2860e-01, 7.4956e-01, 2.2376e-01,\n",
      "        4.6015e-01, 2.9780e-01, 2.7984e-01, 7.7749e-02, 5.4351e-01, 5.5556e-01,\n",
      "        1.3734e-01, 9.9800e-01, 7.8166e-01, 1.4343e-01, 7.9232e-01, 4.6653e-01,\n",
      "        6.9766e-01, 4.3966e-01, 3.3990e-01, 5.1723e-01, 3.5712e-01, 3.5926e-01,\n",
      "        2.5272e-01, 2.9871e-01, 1.3835e-02, 3.3070e-01, 5.0385e-01, 3.4005e-01,\n",
      "        6.5593e-03, 2.5738e-01, 3.4480e-01, 3.9158e-01, 3.1183e-02, 4.2747e-01,\n",
      "        8.1857e-01, 5.9043e-01, 2.3611e-01, 5.1525e-02, 7.0220e-01, 8.1054e-01,\n",
      "        5.9418e-01, 8.2800e-02, 3.9905e-01, 2.0908e-01, 8.6825e-01, 5.4579e-01,\n",
      "        7.8222e-01, 2.4196e-01, 6.7987e-01, 6.8759e-01, 3.8209e-01, 9.0517e-01,\n",
      "        7.0637e-01, 4.2422e-01, 2.5004e-01, 8.8615e-02, 9.6788e-01, 3.2661e-01,\n",
      "        7.6979e-01, 2.8656e-02, 2.8095e-01, 6.7491e-01, 3.7409e-01, 1.5398e-01,\n",
      "        6.8902e-01, 1.8164e-01, 2.3251e-01, 9.9097e-01, 8.3057e-01, 8.6813e-01,\n",
      "        2.1094e-01, 2.9466e-01, 6.1405e-01, 2.7577e-01, 8.7542e-01, 8.8789e-01,\n",
      "        3.3492e-01, 4.2060e-01, 1.2528e-01, 1.0712e-01, 9.3303e-02, 1.0241e-01,\n",
      "        3.6168e-01, 4.3961e-01, 9.5297e-01, 4.5387e-01, 5.1738e-01, 4.5475e-01,\n",
      "        6.0649e-01, 2.5812e-01, 8.9793e-01, 2.8725e-01, 3.2945e-02, 7.8235e-01,\n",
      "        2.4833e-01, 6.7171e-01, 3.6124e-01, 7.5598e-01, 1.4036e-01, 5.7690e-01,\n",
      "        4.2453e-01, 8.9168e-01, 6.2873e-01, 5.5910e-01, 3.2683e-01, 4.6835e-01,\n",
      "        2.2258e-01, 2.0951e-01, 2.0021e-02, 3.4082e-01, 5.7649e-01, 2.3623e-01,\n",
      "        8.7267e-02, 4.2721e-01, 6.4758e-02, 6.4125e-01, 7.9245e-01, 9.4316e-01,\n",
      "        8.9769e-01, 7.9261e-01, 1.8001e-01, 6.2548e-01, 8.8384e-01, 8.7519e-02,\n",
      "        8.2690e-01, 8.0259e-01, 2.7782e-01, 4.0804e-01, 2.8093e-01, 5.2592e-01,\n",
      "        8.2654e-02, 3.1581e-01, 8.8179e-01, 1.5158e-01, 1.4399e-01, 9.7779e-01,\n",
      "        6.8057e-02, 4.6625e-01, 5.2128e-01, 3.8511e-02, 3.3265e-01, 5.3601e-01,\n",
      "        2.6140e-01, 5.9117e-01, 8.3982e-01, 4.5828e-01, 4.6430e-01, 4.2496e-01,\n",
      "        6.8139e-01, 1.0700e-01, 7.8634e-01, 4.1604e-02, 1.3054e-01, 7.4787e-01,\n",
      "        1.6593e-01, 5.8810e-01, 7.8409e-01, 7.8952e-01, 1.8775e-01, 8.5490e-01,\n",
      "        8.5827e-02, 6.4149e-01, 3.3386e-01, 4.4808e-01, 9.2147e-01, 6.9097e-02,\n",
      "        4.2428e-01, 6.0664e-01, 5.3847e-01, 5.9590e-01, 8.0744e-01, 9.6868e-01,\n",
      "        9.7586e-01, 3.5266e-01, 5.4147e-01, 9.4296e-01, 6.4542e-01, 5.6685e-02,\n",
      "        3.3742e-01, 6.3666e-01, 1.2865e-01, 6.9868e-01, 2.9295e-01, 2.8135e-01,\n",
      "        2.5378e-01, 6.6366e-01, 2.0040e-01, 1.4122e-02, 2.6746e-01, 7.5456e-01,\n",
      "        8.6305e-01, 8.4529e-01, 9.8338e-02, 4.9083e-01, 4.3311e-01, 2.3741e-01,\n",
      "        8.3957e-01, 6.6514e-01, 4.6935e-01, 6.9263e-02, 2.7424e-01, 5.3642e-01,\n",
      "        7.7704e-01, 9.5995e-01, 5.9417e-01, 7.6699e-01, 6.7171e-01, 8.6174e-01,\n",
      "        6.0581e-01, 8.3798e-01, 6.1369e-04, 2.2655e-03, 6.2870e-01, 5.5742e-01,\n",
      "        2.2177e-01, 1.8665e-01, 9.0210e-01, 5.7357e-01, 4.8982e-01, 7.0141e-01,\n",
      "        4.0568e-01, 7.9747e-01, 6.4023e-01, 9.9074e-01, 5.5747e-01, 9.4201e-01,\n",
      "        2.0987e-01, 8.6069e-01, 8.9170e-01, 7.3947e-01, 9.8744e-01, 7.3671e-01,\n",
      "        3.7685e-01, 3.3886e-01, 1.6364e-01, 1.0655e-01, 1.5911e-01, 5.9776e-01,\n",
      "        8.1565e-01, 2.5988e-01, 2.7104e-01, 2.0829e-01, 7.2656e-01, 4.5759e-01,\n",
      "        1.6347e-02, 9.3945e-01, 5.8257e-01, 3.1162e-01, 2.5531e-01, 7.1598e-01,\n",
      "        1.3799e-01, 7.0571e-01, 2.4699e-01, 6.5724e-01, 9.9209e-01, 2.9223e-01,\n",
      "        7.1156e-01, 9.0932e-01, 3.5302e-01, 6.6321e-01, 2.7144e-01, 7.7570e-01,\n",
      "        6.0047e-01, 7.5582e-01, 3.0956e-01, 2.3736e-01, 2.8982e-01, 6.9667e-01,\n",
      "        8.3824e-01, 8.2609e-02, 2.8963e-01, 9.8037e-01, 9.0403e-01, 1.0651e-01,\n",
      "        4.1813e-02, 7.7751e-01, 7.5224e-01, 9.7484e-01, 1.2232e-01, 9.4993e-01,\n",
      "        8.2100e-01, 6.6458e-01, 8.3223e-01, 3.9149e-01, 3.7039e-01, 2.8542e-01,\n",
      "        7.6212e-01, 7.6902e-01, 5.8347e-01, 4.9424e-01, 9.0952e-01, 6.6946e-01,\n",
      "        6.4613e-02, 6.2044e-01, 9.8047e-01, 3.0169e-01, 7.1014e-01, 8.0912e-01,\n",
      "        3.5804e-01, 8.0479e-01, 6.8581e-01, 1.5473e-01, 3.9668e-01, 1.1362e-01,\n",
      "        8.5389e-02, 4.6248e-01, 2.0067e-01, 2.3332e-01, 4.3201e-01, 2.9101e-01,\n",
      "        3.3618e-01, 3.0084e-01, 7.4039e-01, 3.7599e-01, 3.2581e-01, 4.0280e-01,\n",
      "        3.3494e-01, 4.6215e-01, 4.8363e-01, 4.5374e-01, 2.9663e-01, 7.0800e-01,\n",
      "        8.7504e-01, 6.3418e-01, 7.0311e-01, 2.9854e-01, 5.3032e-01, 8.6712e-01,\n",
      "        1.8692e-01, 5.4243e-01, 5.4051e-01, 9.3133e-01, 6.3781e-01, 2.4243e-03,\n",
      "        2.6996e-01, 8.2052e-01, 5.5121e-01, 8.8352e-01, 2.0963e-01, 1.8054e-01,\n",
      "        7.6591e-01, 6.0172e-01, 3.0853e-01, 1.1934e-01, 4.8936e-01, 1.4710e-01,\n",
      "        6.6968e-01, 4.7765e-01, 5.9561e-01, 7.0204e-01, 9.4680e-01, 6.6930e-01,\n",
      "        7.7223e-01, 8.4024e-01, 5.8650e-01, 2.0853e-01, 5.8464e-01, 9.9787e-01,\n",
      "        9.0744e-01, 3.6035e-01, 3.8279e-01, 4.9937e-01, 7.4796e-01, 1.0692e-01,\n",
      "        1.8481e-01, 2.0525e-01, 8.6776e-01, 9.5999e-01, 6.7152e-01, 1.2594e-01,\n",
      "        9.9515e-01, 5.3071e-02, 3.6295e-01, 6.9751e-01, 7.2449e-01, 2.7499e-01,\n",
      "        9.8105e-01, 5.3194e-02, 4.7413e-01, 5.7153e-01, 1.1501e-01, 5.0073e-01,\n",
      "        8.0286e-01, 8.5366e-01, 8.6868e-01, 7.2944e-01, 5.7016e-01, 6.7015e-01,\n",
      "        4.1509e-02, 2.6012e-01, 6.6901e-02, 6.0187e-01, 7.4332e-03, 6.5851e-01,\n",
      "        7.1452e-02, 4.3012e-02, 8.9619e-02, 2.8501e-01, 7.2300e-01, 6.6423e-02,\n",
      "        6.5816e-01, 2.7324e-01, 5.0576e-01, 9.6895e-01, 6.6617e-01, 7.4347e-02,\n",
      "        8.8955e-01, 8.7000e-01, 1.3731e-01, 1.4703e-01, 7.7119e-01, 3.8417e-02,\n",
      "        4.5121e-01, 3.1992e-01, 1.7277e-01, 5.6186e-01, 2.8203e-01, 7.1082e-02,\n",
      "        3.1760e-01, 5.8354e-01, 6.1506e-01, 8.5737e-01, 2.8733e-01, 1.3044e-01,\n",
      "        2.7723e-01, 7.4779e-01, 6.1539e-01, 2.5577e-01, 3.3307e-01, 1.9022e-01,\n",
      "        8.9142e-01, 7.0889e-01, 2.5683e-01, 8.2075e-01, 4.7216e-01, 1.9120e-02,\n",
      "        3.1664e-01, 4.3875e-01, 7.4610e-01, 7.2536e-01, 2.6494e-01, 7.5727e-01,\n",
      "        1.7053e-01, 1.5593e-01, 7.9616e-01, 7.4838e-01, 6.3983e-01, 2.5035e-01,\n",
      "        2.0093e-01, 7.2102e-01, 3.6987e-01, 4.8227e-01, 2.4133e-01, 9.8108e-01,\n",
      "        9.1952e-01, 4.0623e-01, 6.9094e-01, 3.8106e-02, 2.2849e-01, 6.3940e-01,\n",
      "        9.8448e-01, 5.6019e-01, 9.9205e-01, 1.2255e-01, 8.0971e-01, 7.8105e-01,\n",
      "        2.8367e-01, 6.7331e-01, 5.7905e-01, 5.6963e-02, 1.6488e-01, 6.9600e-01,\n",
      "        6.0501e-01, 9.4918e-01, 9.0599e-01, 6.1638e-01, 8.8551e-01, 4.2732e-01,\n",
      "        1.7187e-01, 5.8675e-01, 5.8119e-01, 7.1160e-01, 7.0754e-01, 5.8696e-01,\n",
      "        5.2387e-04, 8.5639e-02, 1.0566e-01, 9.5271e-01, 2.5075e-01, 7.8242e-01,\n",
      "        7.9802e-01, 1.9967e-01, 4.6198e-01, 6.6462e-01, 8.6712e-01, 1.7104e-01],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 56\u001b[0m\n\u001b[1;32m     50\u001b[0m  optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# print(scale)\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m  grad_norm \u001b[38;5;241m=\u001b[39m \u001b[43mget_grad_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# print(\"Parameters:\", [p for p in model.parameters() if p.grad is not False])\u001b[39;00m\n\u001b[1;32m     59\u001b[0m  \n\u001b[1;32m     60\u001b[0m \n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m  \u001b[38;5;66;03m# update gumbel temperature\u001b[39;00m\n\u001b[1;32m     63\u001b[0m  gumbel_temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m     64\u001b[0m              max_gumbel_temperature \u001b[38;5;241m*\u001b[39m gumbel_temperature_decay\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcompleted_steps,\n\u001b[1;32m     65\u001b[0m              min_gumbel_temperature,\n\u001b[1;32m     66\u001b[0m      )\n",
      "Cell \u001b[0;32mIn[2], line 80\u001b[0m, in \u001b[0;36mget_grad_norm\u001b[0;34m(params, scale)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot none\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28mprint\u001b[39m(p)\n\u001b[0;32m---> 80\u001b[0m         param_norm \u001b[38;5;241m=\u001b[39m (\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m()\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m/\u001b[39m scale)\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     81\u001b[0m         total_norm \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m param_norm\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     82\u001b[0m total_norm \u001b[38;5;241m=\u001b[39m total_norm\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(max_train_steps))\n",
    "completed_steps = 0\n",
    "starting_epoch = 0\n",
    "\n",
    "for epoch in range(starting_epoch,num_train_epochs):\n",
    "    model.train()\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        num_losses = batch[\"mask_time_indices\"].sum()\n",
    "        \n",
    "        sub_attention_mask = batch.pop('sub_attention_mask',None)\n",
    "        sub_attention_mask = (\n",
    "                sub_attention_mask if sub_attention_mask is not None else torch.ones_like(batch[\"mask_time_indices\"])\n",
    "            )\n",
    "        if sub_attention_mask is None:\n",
    "             print('HERE')\n",
    "        percent_masked = num_losses / sub_attention_mask.sum()\n",
    "\n",
    "\n",
    "        outputs = model(**batch)\n",
    "\n",
    "\n",
    "       # print(outputs.loss)\n",
    "\n",
    "      \n",
    "\n",
    "        loss = outputs.loss\n",
    "\n",
    "        \n",
    "      #  print(\"Loss value:\", loss.item())\n",
    "      #  print(\"Requires grad:\", loss.requires_grad)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        #print(\"Gradients after backward pass:\", [p.grad for p in model.parameters() if p.grad is not None])\n",
    "\n",
    "        # if all(p.grad is None for p in model.parameters()):\n",
    "        #     print(\"No gradients computed\")\n",
    "        #     continue\n",
    "\n",
    "\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "\n",
    "        multiply_grads(model.parameters(), 1 / num_losses)\n",
    "\n",
    "        if (step + 1) % 1 == 0 or step == len(train_dataloader) - 1:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            optimizer.step()\n",
    "            \n",
    "         \n",
    "\n",
    "           # print(scale)\n",
    "\n",
    "            grad_norm = get_grad_norm(model.parameters())\n",
    "\n",
    "           # print(\"Parameters:\", [p for p in model.parameters() if p.grad is not False])\n",
    "            \n",
    "\n",
    "     \n",
    "            # update gumbel temperature\n",
    "            gumbel_temperature = max(\n",
    "                        max_gumbel_temperature * gumbel_temperature_decay**completed_steps,\n",
    "                        min_gumbel_temperature,\n",
    "                )\n",
    "            if hasattr(model, \"module\"):\n",
    "                    model.module.set_gumbel_temperature(gumbel_temperature)\n",
    "            else:\n",
    "                    model.set_gumbel_temperature(gumbel_temperature)\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            completed_steps += 1\n",
    "\n",
    "        if (step + 1) % (1 * logging_steps) == 0:\n",
    "            loss.detach()\n",
    "            outputs.contrastive_loss.detach()\n",
    "            outputs.diversity_loss.detach()\n",
    "\n",
    "            train_logs = {\n",
    "                    \"loss\": (loss * gradient_accumulation_steps) / num_losses,\n",
    "                    \"constrast_loss\": outputs.contrastive_loss / num_losses,\n",
    "                    \"div_loss\": outputs.diversity_loss / num_losses,\n",
    "                    \"%_mask_idx\": percent_masked / 1,\n",
    "                    \"ppl\": outputs.codevector_perplexity,\n",
    "                    \"lr\": torch.tensor(optimizer.param_groups[0][\"lr\"]),\n",
    "                    \"temp\": torch.tensor(gumbel_temperature),\n",
    "                    \"grad_norm\": torch.tensor(grad_norm),\n",
    "            }\n",
    "            log_str = \"\"\n",
    "            for k, v in train_logs.items():\n",
    "                log_str += \"| {}: {:.3e}\".format(k, v.item())\n",
    "\n",
    "            if True:\n",
    "                progress_bar.write(log_str)\n",
    "                for k, v in train_logs.items():\n",
    "                    writer.add_scalar(f'train/{k}', v.item(), completed_steps)\n",
    "        \n",
    "    \n",
    "\n",
    "        if completed_steps >= max_train_steps:\n",
    "                break\n",
    "        \n",
    "\n",
    "    # 7. Validate!\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # init logs\n",
    "    val_logs = {\n",
    "        \"val_loss\": 0,\n",
    "        \"val_contrastive_loss\": 0,\n",
    "        \"val_diversity_loss\": 0,\n",
    "        \"val_num_losses\": 0,\n",
    "    }\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            batch.pop(\"sub_attention_mask\", None)\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        val_logs[\"val_loss\"] += outputs.loss\n",
    "        val_logs[\"val_contrastive_loss\"] += outputs.contrastive_loss\n",
    "        val_logs[\"val_diversity_loss\"] += outputs.diversity_loss\n",
    "        val_logs[\"val_num_losses\"] += batch[\"mask_time_indices\"].sum()\n",
    "\n",
    "\n",
    "    val_logs = {k: v / val_logs[\"val_num_losses\"] for k, v in val_logs.items()}\n",
    "\n",
    "    log_str = \"\"\n",
    "    for k, v in val_logs.items():\n",
    "        log_str += \"| {}: {:.3e}\".format(k, v.item())\n",
    "\n",
    "    if accelerator.is_local_main_process:\n",
    "        progress_bar.write(log_str)\n",
    "        # Log validation metrics to TensorBoard\n",
    "        for k, v in val_logs.items():\n",
    "            writer.add_scalar(f'val/{k}', v.item(), epoch)\n",
    "        \n",
    "    if output_dir is not None:\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(\n",
    "            output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "                \n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad_norm(params, scale=1):\n",
    "    \"\"\"Compute grad norm given a gradient scale.\"\"\"\n",
    "    total_norm = 0.0\n",
    "    for p in params:\n",
    "        if p.grad is not False:\n",
    "            print('not none')\n",
    "            print(p)\n",
    "            param_norm = (p.grad.detach().data / scale).norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm**0.5\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    if p.grad is not False:\n",
    "        print(p.grad)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not none\n",
      "Parameter containing:\n",
      "tensor([8.4824e-02, 9.8099e-01, 4.5328e-01, 4.3967e-01, 5.0145e-01, 1.8659e-01,\n",
      "        2.7512e-01, 2.2131e-02, 1.4914e-01, 1.1665e-01, 2.8491e-01, 4.4491e-01,\n",
      "        9.8502e-01, 3.4413e-01, 2.9262e-01, 1.1975e-01, 4.6609e-01, 1.9719e-01,\n",
      "        4.1189e-01, 3.7040e-01, 5.7756e-01, 3.1124e-01, 8.7557e-01, 7.7711e-02,\n",
      "        3.9245e-01, 2.3488e-01, 8.5205e-01, 8.7894e-01, 3.0212e-01, 7.3275e-02,\n",
      "        7.8919e-01, 6.7912e-01, 2.2240e-01, 2.6755e-01, 5.2048e-01, 1.6625e-01,\n",
      "        7.6600e-01, 7.9738e-01, 2.7697e-01, 2.3656e-01, 7.1059e-01, 1.8312e-01,\n",
      "        5.6537e-01, 7.8787e-01, 6.5091e-01, 4.5836e-01, 6.0524e-01, 8.9491e-02,\n",
      "        2.2072e-01, 3.9741e-01, 6.5188e-01, 4.2748e-01, 3.5178e-01, 2.7553e-01,\n",
      "        9.9817e-01, 5.9067e-01, 1.4937e-02, 9.8916e-01, 9.2044e-01, 5.5102e-01,\n",
      "        8.6186e-01, 4.1171e-01, 5.8003e-01, 3.1871e-01, 6.7465e-02, 7.6799e-01,\n",
      "        9.1963e-01, 9.1382e-01, 2.5272e-02, 2.3235e-01, 7.9589e-01, 1.1008e-01,\n",
      "        9.3207e-01, 9.0076e-01, 9.9762e-01, 7.8295e-01, 7.8196e-01, 3.4258e-01,\n",
      "        6.5594e-01, 5.6514e-01, 1.5807e-01, 6.7778e-01, 4.5055e-01, 7.8271e-01,\n",
      "        9.5349e-01, 6.7738e-01, 7.6623e-02, 4.1282e-01, 3.5178e-01, 9.1333e-01,\n",
      "        6.1644e-01, 9.0903e-01, 7.2996e-01, 9.7282e-01, 9.8811e-01, 8.1825e-01,\n",
      "        1.5614e-01, 4.5992e-01, 2.8517e-03, 7.5282e-01, 6.7197e-01, 3.6828e-01,\n",
      "        6.0925e-02, 8.2621e-01, 2.7115e-01, 5.1838e-01, 8.5919e-01, 5.0917e-03,\n",
      "        4.9071e-01, 3.5263e-01, 5.2175e-01, 3.2155e-01, 7.7749e-01, 7.8121e-01,\n",
      "        4.6464e-01, 9.0407e-01, 2.6732e-02, 8.2510e-01, 9.7426e-01, 2.1628e-02,\n",
      "        1.1867e-01, 3.4969e-01, 6.1594e-01, 2.1138e-01, 5.2954e-02, 6.3628e-01,\n",
      "        8.8924e-01, 8.1124e-02, 2.8572e-01, 2.9905e-01, 1.9385e-01, 7.7757e-01,\n",
      "        7.6841e-01, 5.8828e-01, 1.4606e-01, 9.0400e-01, 6.6971e-01, 4.2185e-01,\n",
      "        9.3127e-01, 6.1895e-01, 1.9815e-01, 4.3115e-01, 7.9264e-01, 4.8293e-02,\n",
      "        3.2750e-01, 5.9981e-01, 4.5029e-02, 8.4083e-01, 7.1817e-02, 3.5725e-01,\n",
      "        5.7307e-01, 5.1349e-02, 2.0241e-01, 3.4125e-01, 3.6925e-02, 6.0905e-01,\n",
      "        5.7269e-01, 6.2385e-01, 7.2230e-01, 6.8858e-01, 3.6462e-01, 5.5074e-02,\n",
      "        8.1310e-01, 6.9931e-01, 1.0261e-01, 6.1047e-01, 8.0528e-01, 7.3517e-01,\n",
      "        5.6925e-01, 6.2792e-01, 2.8206e-01, 1.1562e-01, 6.4785e-02, 1.9619e-01,\n",
      "        5.2544e-02, 7.2354e-02, 3.5719e-01, 2.6656e-01, 4.5285e-01, 5.8557e-01,\n",
      "        7.3742e-01, 7.3713e-01, 8.2214e-01, 1.8275e-01, 8.9718e-01, 1.2518e-01,\n",
      "        1.4717e-02, 6.1257e-01, 4.7074e-01, 5.5295e-01, 6.1132e-01, 9.2406e-01,\n",
      "        7.0617e-01, 7.0760e-01, 8.8681e-01, 1.0890e-01, 8.6615e-01, 4.0680e-01,\n",
      "        4.9083e-01, 1.5538e-01, 8.6443e-01, 2.3995e-01, 8.7223e-01, 7.0661e-01,\n",
      "        9.7449e-01, 4.7516e-01, 5.1597e-01, 1.6355e-01, 2.1695e-01, 3.8407e-01,\n",
      "        7.3833e-01, 1.0650e-02, 1.7912e-01, 5.4048e-01, 2.0273e-01, 7.7593e-02,\n",
      "        9.3677e-01, 9.0170e-01, 5.7183e-01, 7.8415e-01, 3.4343e-01, 7.4040e-01,\n",
      "        5.0922e-01, 2.7589e-01, 3.4505e-01, 9.9278e-01, 1.7058e-01, 4.7474e-01,\n",
      "        6.0744e-01, 3.5933e-01, 1.0362e-02, 1.0726e-01, 6.3330e-01, 3.0575e-01,\n",
      "        9.2735e-01, 7.6550e-01, 4.5483e-01, 5.6050e-02, 6.8423e-01, 4.0734e-01,\n",
      "        7.5402e-01, 8.6286e-01, 1.7934e-01, 8.6081e-01, 6.3106e-01, 7.1914e-01,\n",
      "        3.7378e-03, 3.7118e-01, 2.9971e-01, 3.9911e-01, 1.3674e-01, 3.6431e-01,\n",
      "        7.1475e-01, 7.5816e-01, 4.2078e-01, 6.2534e-01, 1.7065e-01, 8.2430e-01,\n",
      "        7.5787e-01, 2.0550e-01, 6.8330e-01, 3.4750e-01, 3.1454e-01, 1.8342e-01,\n",
      "        4.0272e-01, 8.9292e-01, 9.8934e-01, 6.2846e-01, 6.2540e-01, 3.4773e-01,\n",
      "        3.0719e-01, 4.0468e-01, 2.6591e-01, 9.2860e-01, 7.4956e-01, 2.2376e-01,\n",
      "        4.6015e-01, 2.9780e-01, 2.7984e-01, 7.7749e-02, 5.4351e-01, 5.5556e-01,\n",
      "        1.3734e-01, 9.9800e-01, 7.8166e-01, 1.4343e-01, 7.9232e-01, 4.6653e-01,\n",
      "        6.9766e-01, 4.3966e-01, 3.3990e-01, 5.1723e-01, 3.5712e-01, 3.5926e-01,\n",
      "        2.5272e-01, 2.9871e-01, 1.3835e-02, 3.3070e-01, 5.0385e-01, 3.4005e-01,\n",
      "        6.5593e-03, 2.5738e-01, 3.4480e-01, 3.9158e-01, 3.1183e-02, 4.2747e-01,\n",
      "        8.1857e-01, 5.9043e-01, 2.3611e-01, 5.1525e-02, 7.0220e-01, 8.1054e-01,\n",
      "        5.9418e-01, 8.2800e-02, 3.9905e-01, 2.0908e-01, 8.6825e-01, 5.4579e-01,\n",
      "        7.8222e-01, 2.4196e-01, 6.7987e-01, 6.8759e-01, 3.8209e-01, 9.0517e-01,\n",
      "        7.0637e-01, 4.2422e-01, 2.5004e-01, 8.8615e-02, 9.6788e-01, 3.2661e-01,\n",
      "        7.6979e-01, 2.8656e-02, 2.8095e-01, 6.7491e-01, 3.7409e-01, 1.5398e-01,\n",
      "        6.8902e-01, 1.8164e-01, 2.3251e-01, 9.9097e-01, 8.3057e-01, 8.6813e-01,\n",
      "        2.1094e-01, 2.9466e-01, 6.1405e-01, 2.7577e-01, 8.7542e-01, 8.8789e-01,\n",
      "        3.3492e-01, 4.2060e-01, 1.2528e-01, 1.0712e-01, 9.3303e-02, 1.0241e-01,\n",
      "        3.6168e-01, 4.3961e-01, 9.5297e-01, 4.5387e-01, 5.1738e-01, 4.5475e-01,\n",
      "        6.0649e-01, 2.5812e-01, 8.9793e-01, 2.8725e-01, 3.2945e-02, 7.8235e-01,\n",
      "        2.4833e-01, 6.7171e-01, 3.6124e-01, 7.5598e-01, 1.4036e-01, 5.7690e-01,\n",
      "        4.2453e-01, 8.9168e-01, 6.2873e-01, 5.5910e-01, 3.2683e-01, 4.6835e-01,\n",
      "        2.2258e-01, 2.0951e-01, 2.0021e-02, 3.4082e-01, 5.7649e-01, 2.3623e-01,\n",
      "        8.7267e-02, 4.2721e-01, 6.4758e-02, 6.4125e-01, 7.9245e-01, 9.4316e-01,\n",
      "        8.9769e-01, 7.9261e-01, 1.8001e-01, 6.2548e-01, 8.8384e-01, 8.7519e-02,\n",
      "        8.2690e-01, 8.0259e-01, 2.7782e-01, 4.0804e-01, 2.8093e-01, 5.2592e-01,\n",
      "        8.2654e-02, 3.1581e-01, 8.8179e-01, 1.5158e-01, 1.4399e-01, 9.7779e-01,\n",
      "        6.8057e-02, 4.6625e-01, 5.2128e-01, 3.8511e-02, 3.3265e-01, 5.3601e-01,\n",
      "        2.6140e-01, 5.9117e-01, 8.3982e-01, 4.5828e-01, 4.6430e-01, 4.2496e-01,\n",
      "        6.8139e-01, 1.0700e-01, 7.8634e-01, 4.1604e-02, 1.3054e-01, 7.4787e-01,\n",
      "        1.6593e-01, 5.8810e-01, 7.8409e-01, 7.8952e-01, 1.8775e-01, 8.5490e-01,\n",
      "        8.5827e-02, 6.4149e-01, 3.3386e-01, 4.4808e-01, 9.2147e-01, 6.9097e-02,\n",
      "        4.2428e-01, 6.0664e-01, 5.3847e-01, 5.9590e-01, 8.0744e-01, 9.6868e-01,\n",
      "        9.7586e-01, 3.5266e-01, 5.4147e-01, 9.4296e-01, 6.4542e-01, 5.6685e-02,\n",
      "        3.3742e-01, 6.3666e-01, 1.2865e-01, 6.9868e-01, 2.9295e-01, 2.8135e-01,\n",
      "        2.5378e-01, 6.6366e-01, 2.0040e-01, 1.4122e-02, 2.6746e-01, 7.5456e-01,\n",
      "        8.6305e-01, 8.4529e-01, 9.8338e-02, 4.9083e-01, 4.3311e-01, 2.3741e-01,\n",
      "        8.3957e-01, 6.6514e-01, 4.6935e-01, 6.9263e-02, 2.7424e-01, 5.3642e-01,\n",
      "        7.7704e-01, 9.5995e-01, 5.9417e-01, 7.6699e-01, 6.7171e-01, 8.6174e-01,\n",
      "        6.0581e-01, 8.3798e-01, 6.1369e-04, 2.2655e-03, 6.2870e-01, 5.5742e-01,\n",
      "        2.2177e-01, 1.8665e-01, 9.0210e-01, 5.7357e-01, 4.8982e-01, 7.0141e-01,\n",
      "        4.0568e-01, 7.9747e-01, 6.4023e-01, 9.9074e-01, 5.5747e-01, 9.4201e-01,\n",
      "        2.0987e-01, 8.6069e-01, 8.9170e-01, 7.3947e-01, 9.8744e-01, 7.3671e-01,\n",
      "        3.7685e-01, 3.3886e-01, 1.6364e-01, 1.0655e-01, 1.5911e-01, 5.9776e-01,\n",
      "        8.1565e-01, 2.5988e-01, 2.7104e-01, 2.0829e-01, 7.2656e-01, 4.5759e-01,\n",
      "        1.6347e-02, 9.3945e-01, 5.8257e-01, 3.1162e-01, 2.5531e-01, 7.1598e-01,\n",
      "        1.3799e-01, 7.0571e-01, 2.4699e-01, 6.5724e-01, 9.9209e-01, 2.9223e-01,\n",
      "        7.1156e-01, 9.0932e-01, 3.5302e-01, 6.6321e-01, 2.7144e-01, 7.7570e-01,\n",
      "        6.0047e-01, 7.5582e-01, 3.0956e-01, 2.3736e-01, 2.8982e-01, 6.9667e-01,\n",
      "        8.3824e-01, 8.2609e-02, 2.8963e-01, 9.8037e-01, 9.0403e-01, 1.0651e-01,\n",
      "        4.1813e-02, 7.7751e-01, 7.5224e-01, 9.7484e-01, 1.2232e-01, 9.4993e-01,\n",
      "        8.2100e-01, 6.6458e-01, 8.3223e-01, 3.9149e-01, 3.7039e-01, 2.8542e-01,\n",
      "        7.6212e-01, 7.6902e-01, 5.8347e-01, 4.9424e-01, 9.0952e-01, 6.6946e-01,\n",
      "        6.4613e-02, 6.2044e-01, 9.8047e-01, 3.0169e-01, 7.1014e-01, 8.0912e-01,\n",
      "        3.5804e-01, 8.0479e-01, 6.8581e-01, 1.5473e-01, 3.9668e-01, 1.1362e-01,\n",
      "        8.5389e-02, 4.6248e-01, 2.0067e-01, 2.3332e-01, 4.3201e-01, 2.9101e-01,\n",
      "        3.3618e-01, 3.0084e-01, 7.4039e-01, 3.7599e-01, 3.2581e-01, 4.0280e-01,\n",
      "        3.3494e-01, 4.6215e-01, 4.8363e-01, 4.5374e-01, 2.9663e-01, 7.0800e-01,\n",
      "        8.7504e-01, 6.3418e-01, 7.0311e-01, 2.9854e-01, 5.3032e-01, 8.6712e-01,\n",
      "        1.8692e-01, 5.4243e-01, 5.4051e-01, 9.3133e-01, 6.3781e-01, 2.4243e-03,\n",
      "        2.6996e-01, 8.2052e-01, 5.5121e-01, 8.8352e-01, 2.0963e-01, 1.8054e-01,\n",
      "        7.6591e-01, 6.0172e-01, 3.0853e-01, 1.1934e-01, 4.8936e-01, 1.4710e-01,\n",
      "        6.6968e-01, 4.7765e-01, 5.9561e-01, 7.0204e-01, 9.4680e-01, 6.6930e-01,\n",
      "        7.7223e-01, 8.4024e-01, 5.8650e-01, 2.0853e-01, 5.8464e-01, 9.9787e-01,\n",
      "        9.0744e-01, 3.6035e-01, 3.8279e-01, 4.9937e-01, 7.4796e-01, 1.0692e-01,\n",
      "        1.8481e-01, 2.0525e-01, 8.6776e-01, 9.5999e-01, 6.7152e-01, 1.2594e-01,\n",
      "        9.9515e-01, 5.3071e-02, 3.6295e-01, 6.9751e-01, 7.2449e-01, 2.7499e-01,\n",
      "        9.8105e-01, 5.3194e-02, 4.7413e-01, 5.7153e-01, 1.1501e-01, 5.0073e-01,\n",
      "        8.0286e-01, 8.5366e-01, 8.6868e-01, 7.2944e-01, 5.7016e-01, 6.7015e-01,\n",
      "        4.1509e-02, 2.6012e-01, 6.6901e-02, 6.0187e-01, 7.4332e-03, 6.5851e-01,\n",
      "        7.1452e-02, 4.3012e-02, 8.9619e-02, 2.8501e-01, 7.2300e-01, 6.6423e-02,\n",
      "        6.5816e-01, 2.7324e-01, 5.0576e-01, 9.6895e-01, 6.6617e-01, 7.4347e-02,\n",
      "        8.8955e-01, 8.7000e-01, 1.3731e-01, 1.4703e-01, 7.7119e-01, 3.8417e-02,\n",
      "        4.5121e-01, 3.1992e-01, 1.7277e-01, 5.6186e-01, 2.8203e-01, 7.1082e-02,\n",
      "        3.1760e-01, 5.8354e-01, 6.1506e-01, 8.5737e-01, 2.8733e-01, 1.3044e-01,\n",
      "        2.7723e-01, 7.4779e-01, 6.1539e-01, 2.5577e-01, 3.3307e-01, 1.9022e-01,\n",
      "        8.9142e-01, 7.0889e-01, 2.5683e-01, 8.2075e-01, 4.7216e-01, 1.9120e-02,\n",
      "        3.1664e-01, 4.3875e-01, 7.4610e-01, 7.2536e-01, 2.6494e-01, 7.5727e-01,\n",
      "        1.7053e-01, 1.5593e-01, 7.9616e-01, 7.4838e-01, 6.3983e-01, 2.5035e-01,\n",
      "        2.0093e-01, 7.2102e-01, 3.6987e-01, 4.8227e-01, 2.4133e-01, 9.8108e-01,\n",
      "        9.1952e-01, 4.0623e-01, 6.9094e-01, 3.8106e-02, 2.2849e-01, 6.3940e-01,\n",
      "        9.8448e-01, 5.6019e-01, 9.9205e-01, 1.2255e-01, 8.0971e-01, 7.8105e-01,\n",
      "        2.8367e-01, 6.7331e-01, 5.7905e-01, 5.6963e-02, 1.6488e-01, 6.9600e-01,\n",
      "        6.0501e-01, 9.4918e-01, 9.0599e-01, 6.1638e-01, 8.8551e-01, 4.2732e-01,\n",
      "        1.7187e-01, 5.8675e-01, 5.8119e-01, 7.1160e-01, 7.0754e-01, 5.8696e-01,\n",
      "        5.2387e-04, 8.5639e-02, 1.0566e-01, 9.5271e-01, 2.5075e-01, 7.8242e-01,\n",
      "        7.9802e-01, 1.9967e-01, 4.6198e-01, 6.6462e-01, 8.6712e-01, 1.7104e-01],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_grad_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m, in \u001b[0;36mget_grad_norm\u001b[0;34m(params, scale)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot none\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(p)\n\u001b[0;32m----> 8\u001b[0m         param_norm \u001b[38;5;241m=\u001b[39m (\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m()\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m/\u001b[39m scale)\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      9\u001b[0m         total_norm \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m param_norm\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     10\u001b[0m total_norm \u001b[38;5;241m=\u001b[39m total_norm\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "get_grad_norm(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example pretraining \n",
    "https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-pretraining/run_wav2vec2_pretraining_no_trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
