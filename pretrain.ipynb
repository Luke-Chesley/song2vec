{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoFeatureExtractor, Wav2Vec2ForPreTraining,get_scheduler\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import _compute_mask_indices, _sample_negative_indices\n",
    "#from transformers.models.wav2vec2.feature_extraction_wav2vec2 import Wav2Vec2FeatureExtractor\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import math\n",
    "\n",
    "from model import Wav2Vec2ForPreTraining,Wav2Vec2Config,Wav2Vec2FeatureExtractor\n",
    "from dataset import AudioDatasetSplits\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "device = 'cuda'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds_train = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor()\n",
    "config = Wav2Vec2Config()\n",
    "model = Wav2Vec2ForPreTraining(config).to(device)\n",
    "ds_train = AudioDatasetSplits().train\n",
    "#ds_val = AudioDatasetSplits().val\n",
    "\n",
    "# ips = feature_extractor(ds_train[0]['audio'],return_tensors='pt',sampling_rate=ds_train[0]['sample_rate']).input_values\n",
    "# batch_size, raw_sequence_length = ips.shape\n",
    "# sequence_length = model._get_feat_extract_output_lengths(raw_sequence_length).item()\n",
    "\n",
    "# mask_time_indices = _compute_mask_indices(\n",
    "#     shape=(batch_size, sequence_length), mask_prob=model.config.mask_time_prob, mask_length=model.config.mask_time_length\n",
    "# )\n",
    "\n",
    "# sampled_negative_indices = _sample_negative_indices(\n",
    "#     features_shape=(batch_size, sequence_length),\n",
    "#     num_negatives=model.config.num_negatives,\n",
    "#     mask_time_indices=mask_time_indices,\n",
    "# )\n",
    "\n",
    "# mask_time_indices = torch.tensor(data=mask_time_indices, device=ips.device, dtype=torch.long)\n",
    "# sampled_negative_indices = torch.tensor(\n",
    "#     data=sampled_negative_indices, device=ips.device, dtype=torch.long\n",
    "# )\n",
    "\n",
    "# model = model.train()\n",
    "# loss = model(\n",
    "#     ips, mask_time_indices=mask_time_indices, sampled_negative_indices=sampled_negative_indices\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad_norm(params, scale=1):\n",
    "    \"\"\"Compute grad norm given a gradient scale.\"\"\"\n",
    "    total_norm = 0.0\n",
    "    for p in params:\n",
    "        if p.grad is not None:\n",
    "            param_norm = (p.grad.detach().data / scale).norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm**0.5\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(ds_train,batch_size=8) # can put feature extractor in ds\n",
    "#dl_val = DataLoader(ds_val,batch_size=8)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3817e-06,  2.2483e-06,  1.8022e-06,  ...,  2.3067e-01,\n",
       "          2.1962e-01,  2.2706e-01],\n",
       "        [ 3.2028e-02,  5.8347e-02,  5.2269e-02,  ...,  8.1421e-02,\n",
       "          1.1898e-01,  1.6805e-01],\n",
       "        [ 5.0615e-11,  8.9776e-11,  6.9491e-11,  ..., -1.0592e-01,\n",
       "         -1.2491e-01, -1.4430e-01],\n",
       "        ...,\n",
       "        [-7.9215e-08, -3.4389e-07, -3.7246e-07,  ..., -1.4127e-01,\n",
       "         -2.1177e-01, -2.9916e-01],\n",
       "        [ 1.3507e-07,  1.9804e-07,  3.3540e-07,  ...,  3.9985e-01,\n",
       "          3.3562e-01,  2.8834e-01],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.3280e-02,\n",
       "          1.3537e-02,  1.5178e-02]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dl_train))['audio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_accumulation_steps = 4\n",
    "logging_steps = 10\n",
    "max_gumbel_temperature = 2.0\n",
    "min_gumbel_temperature = 0.5\n",
    "gumbel_temperature_decay = 0.999995\n",
    "num_train_epochs = 1\n",
    "num_update_steps_per_epoch = math.ceil(len(dl_train) / gradient_accumulation_steps)\n",
    "\n",
    "max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "adam_beta1 = 0.9\n",
    "adam_beta2 = 0.999\n",
    "adam_epsilon = 1e-4\n",
    "optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=5e-4,\n",
    "        betas=[adam_beta1, adam_beta2],\n",
    "        eps=adam_epsilon,\n",
    "    )\n",
    "\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "        name='linear',\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=110,\n",
    "        num_training_steps=max_train_steps,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| loss: 1.478e+03| constrast_loss: 3.695e+02| div_loss: 1.676e-01| ppl: 6.387e+02| lr: 1.818e-04| temp: 2.000e+00| grad_norm: 5.689e+01\n",
      "| loss: 2.963e+03| constrast_loss: 7.408e+02| div_loss: 3.909e-01| ppl: 6.384e+02| lr: 3.636e-04| temp: 1.999e+00| grad_norm: 1.040e+02\n",
      "| loss: 2.911e+03| constrast_loss: 7.277e+02| div_loss: 3.884e-01| ppl: 6.384e+02| lr: 4.820e-04| temp: 1.999e+00| grad_norm: 1.313e+02\n",
      "| loss: 2.962e+03| constrast_loss: 7.404e+02| div_loss: 2.032e-01| ppl: 6.392e+02| lr: 4.101e-04| temp: 1.998e+00| grad_norm: 8.022e+01\n",
      "| loss: 2.961e+03| constrast_loss: 7.403e+02| div_loss: 1.280e-01| ppl: 6.395e+02| lr: 3.381e-04| temp: 1.998e+00| grad_norm: 8.202e+01\n",
      "| loss: 2.968e+03| constrast_loss: 7.420e+02| div_loss: 1.232e-01| ppl: 6.395e+02| lr: 2.662e-04| temp: 1.998e+00| grad_norm: 1.190e+02\n",
      "| loss: 2.922e+03| constrast_loss: 7.304e+02| div_loss: 6.827e-02| ppl: 6.397e+02| lr: 1.942e-04| temp: 1.997e+00| grad_norm: 6.009e+01\n",
      "| loss: 2.754e+03| constrast_loss: 6.886e+02| div_loss: 6.497e-02| ppl: 6.397e+02| lr: 1.223e-04| temp: 1.997e+00| grad_norm: 5.202e+01\n",
      "| loss: 1.476e+03| constrast_loss: 3.690e+02| div_loss: 3.485e-02| ppl: 6.397e+02| lr: 5.036e-05| temp: 1.996e+00| grad_norm: 3.707e+01\n",
      "| loss: 2.955e+03| constrast_loss: 7.388e+02| div_loss: 3.192e-02| ppl: 6.399e+02| lr: 0.000e+00| temp: 1.996e+00| grad_norm: 5.009e+01\n",
      "| loss: 1.477e+03| constrast_loss: 3.693e+02| div_loss: 3.876e-02| ppl: 6.397e+02| lr: 0.000e+00| temp: 1.996e+00| grad_norm: 3.625e+01\n",
      "| loss: 2.954e+03| constrast_loss: 7.385e+02| div_loss: 3.687e-02| ppl: 6.399e+02| lr: 0.000e+00| temp: 1.995e+00| grad_norm: 4.990e+01\n",
      "| loss: 2.936e+03| constrast_loss: 7.339e+02| div_loss: 6.502e-02| ppl: 6.397e+02| lr: 0.000e+00| temp: 1.995e+00| grad_norm: 5.172e+01\n",
      "| loss: 1.478e+03| constrast_loss: 3.695e+02| div_loss: 3.062e-02| ppl: 6.398e+02| lr: 0.000e+00| temp: 1.994e+00| grad_norm: 3.532e+01\n",
      "| loss: 2.881e+03| constrast_loss: 7.202e+02| div_loss: 2.830e-02| ppl: 6.399e+02| lr: 0.000e+00| temp: 1.994e+00| grad_norm: 4.973e+01\n",
      "| loss: 2.828e+03| constrast_loss: 7.071e+02| div_loss: 3.327e-02| ppl: 6.399e+02| lr: 0.000e+00| temp: 1.994e+00| grad_norm: 5.290e+01\n",
      "| loss: 2.953e+03| constrast_loss: 7.382e+02| div_loss: 4.073e-02| ppl: 6.398e+02| lr: 0.000e+00| temp: 1.993e+00| grad_norm: 5.158e+01\n",
      "| loss: 1.478e+03| constrast_loss: 3.695e+02| div_loss: 4.393e-02| ppl: 6.396e+02| lr: 0.000e+00| temp: 1.993e+00| grad_norm: 3.824e+01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#batch_iterator = tqdm(dl_train,desc=f\"Processing Epoch {epoch:02d}\")\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx,batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dl_train):\n\u001b[1;32m     10\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     12\u001b[0m   \u001b[38;5;66;03m#  ips = feature_extractor(batch['audio']['array'],sampling_rate=16000,return_tensors='pt').input_values.squeeze(0).to(device)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m    \u001b[38;5;66;03m# print(batch['audio'].shape)\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/jupyterlab/Notebooks/song2vec/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/projects/jupyterlab/Notebooks/song2vec/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/projects/jupyterlab/Notebooks/song2vec/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/projects/jupyterlab/Notebooks/song2vec/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/projects/jupyterlab/Notebooks/song2vec/dataset.py:22\u001b[0m, in \u001b[0;36mAudioDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     21\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_list[idx]\n\u001b[0;32m---> 22\u001b[0m     audio, sample_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresample_audio_torchaudio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_label(file_path)\n\u001b[1;32m     24\u001b[0m     audio \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_audio(audio)\n",
      "File \u001b[0;32m~/projects/jupyterlab/Notebooks/song2vec/dataset.py:32\u001b[0m, in \u001b[0;36mAudioDataset.resample_audio_torchaudio\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresample_audio_torchaudio\u001b[39m(\u001b[38;5;28mself\u001b[39m, file_path):\n\u001b[0;32m---> 32\u001b[0m     waveform, sample_rate \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_rate \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_sample_rate:\n\u001b[1;32m     34\u001b[0m         resampler \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mResample(orig_freq\u001b[38;5;241m=\u001b[39msample_rate, new_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_sample_rate)\n",
      "File \u001b[0;32m~/projects/jupyterlab/Notebooks/song2vec/venv/lib/python3.10/site-packages/torchaudio/_backend/utils.py:205\u001b[0m, in \u001b[0;36mget_load_func.<locals>.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03mBy default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m        `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    204\u001b[0m backend \u001b[38;5;241m=\u001b[39m dispatcher(uri, \u001b[38;5;28mformat\u001b[39m, backend)\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/jupyterlab/Notebooks/song2vec/venv/lib/python3.10/site-packages/torchaudio/_backend/ffmpeg.py:297\u001b[0m, in \u001b[0;36mFFmpegBackend.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m    289\u001b[0m     uri: InputType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    295\u001b[0m     buffer_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4096\u001b[39m,\n\u001b[1;32m    296\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m--> 297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/jupyterlab/Notebooks/song2vec/venv/lib/python3.10/site-packages/torchaudio/_backend/ffmpeg.py:91\u001b[0m, in \u001b[0;36mload_audio\u001b[0;34m(src, frame_offset, num_frames, convert, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m     89\u001b[0m sample_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(s\u001b[38;5;241m.\u001b[39mget_src_stream_info(s\u001b[38;5;241m.\u001b[39mdefault_audio_stream)\u001b[38;5;241m.\u001b[39msample_rate)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mfilter\u001b[39m \u001b[38;5;241m=\u001b[39m _get_load_filter(frame_offset, num_frames, convert)\n\u001b[0;32m---> 91\u001b[0m waveform \u001b[38;5;241m=\u001b[39m \u001b[43m_load_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m waveform, sample_rate\n",
      "File \u001b[0;32m~/projects/jupyterlab/Notebooks/song2vec/venv/lib/python3.10/site-packages/torchaudio/_backend/ffmpeg.py:69\u001b[0m, in \u001b[0;36m_load_audio\u001b[0;34m(s, filter, channels_first)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_audio\u001b[39m(\n\u001b[1;32m     64\u001b[0m     s: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchaudio.io.StreamReader\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mfilter\u001b[39m: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     66\u001b[0m     channels_first: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     67\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     68\u001b[0m     s\u001b[38;5;241m.\u001b[39madd_audio_stream(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, filter_desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfilter\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m     \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_all_packets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mpop_chunks()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunk \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/projects/jupyterlab/Notebooks/song2vec/venv/lib/python3.10/site-packages/torio/io/_streaming_media_decoder.py:901\u001b[0m, in \u001b[0;36mStreamingMediaDecoder.process_all_packets\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_all_packets\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    900\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Process packets until it reaches EOF.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_be\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_all_packets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_epochs = 0\n",
    "completed_steps = 0\n",
    "progress_bar = tqdm(range(max_train_steps),disable=True)\n",
    "\n",
    "for epoch in range(start_epochs,num_train_epochs):\n",
    "    model.train()\n",
    "    #batch_iterator = tqdm(dl_train,desc=f\"Processing Epoch {epoch:02d}\")\n",
    "    for batch_idx,batch in enumerate(dl_train):\n",
    "\n",
    "        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "\n",
    "      #  ips = feature_extractor(batch['audio']['array'],sampling_rate=16000,return_tensors='pt').input_values.squeeze(0).to(device)\n",
    "\n",
    "\n",
    "\n",
    "       # print(batch['audio'].shape)\n",
    "        ips = feature_extractor(batch['audio'],sampling_rate=batch['sample_rate'][0],return_tensors='pt').input_values.squeeze(0).to(device)\n",
    "        #print(ips.shape)\n",
    "\n",
    "        batch_size,raw_sequence_length = ips.shape\n",
    "\n",
    "        sequence_length = model._get_feat_extract_output_lengths(raw_sequence_length).item()\n",
    "\n",
    "\n",
    "        mask_time_indices = _compute_mask_indices(\n",
    "            shape=(batch_size, sequence_length), mask_prob=model.config.mask_time_prob, mask_length=model.config.mask_time_length\n",
    "        )\n",
    "\n",
    "        sampled_negative_indices = _sample_negative_indices(\n",
    "            features_shape=(batch_size, sequence_length),\n",
    "            num_negatives=model.config.num_negatives,\n",
    "            mask_time_indices=mask_time_indices,\n",
    "        )\n",
    "\n",
    "        mask_time_indices = torch.tensor(data=mask_time_indices, device=ips.device, dtype=torch.long)\n",
    "        sampled_negative_indices = torch.tensor(\n",
    "            data=sampled_negative_indices, device=ips.device, dtype=torch.long\n",
    "        )\n",
    "\n",
    "        #print(sampled_negative_indices.shape)\n",
    "\n",
    "       #print(ips)\n",
    "       # print(mask_time_indices)\n",
    "       # print(sampled_negative_indices)\n",
    "\n",
    "        outputs = model(ips,mask_time_indices=mask_time_indices,sampled_negative_indices=sampled_negative_indices)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "       # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        if (batch_idx + 1) % gradient_accumulation_steps == 0 or batch_idx == len(dl_train) - 1:\n",
    "                grad_norm = get_grad_norm(model.parameters(), 1)\n",
    "\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        gumbel_temperature = max(\n",
    "                    max_gumbel_temperature * gumbel_temperature_decay**completed_steps,\n",
    "                    min_gumbel_temperature,\n",
    "                )\n",
    "        \n",
    "        model.set_gumbel_temperature(gumbel_temperature)\n",
    "\n",
    "        completed_steps += 1\n",
    "\n",
    "        # 6. Log all results\n",
    "        if (batch_idx + 1) % (gradient_accumulation_steps * logging_steps) == 0:\n",
    "            loss.detach()\n",
    "            outputs.contrastive_loss.detach()\n",
    "            outputs.diversity_loss.detach()\n",
    "\n",
    "            train_logs = {\n",
    "                \"loss\": (loss * gradient_accumulation_steps) ,\n",
    "                \"constrast_loss\": outputs.contrastive_loss ,\n",
    "                \"div_loss\": outputs.diversity_loss ,\n",
    "                \"ppl\": outputs.codevector_perplexity,\n",
    "                \"lr\": torch.tensor(optimizer.param_groups[0][\"lr\"]),\n",
    "                \"temp\": torch.tensor(gumbel_temperature),\n",
    "                \"grad_norm\": torch.tensor(grad_norm),\n",
    "            }\n",
    "            log_str = \"\"\n",
    "            for k, v in train_logs.items():\n",
    "                log_str += \"| {}: {:.3e}\".format(k, v.item())\n",
    "\n",
    "            progress_bar.write(log_str)\n",
    "\n",
    "            \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #print('loss',loss)\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "        #batch_iterator.set_postfix(loss=running_loss /( 1 if batch_idx == 0 else batch_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3846610266.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    ------------------------------------------------------------------\u001b[0m\n\u001b[0m                                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "audio = 1, 80000\n",
    "tensor([[1.3817e-06, 2.2483e-06, 1.8022e-06,  ..., 2.3067e-01, 2.1962e-01,\n",
    "         2.2706e-01]], device='cuda:0')\n",
    "------------------------------------------------------------------\n",
    "\n",
    "ips = 1, 80000 \n",
    "tensor([[2.2682e-04, 2.3256e-04, 2.2961e-04,  ..., 1.5293e+00, 1.4560e+00,\n",
    "         1.5053e+00]], device='cuda:0')\n",
    "------------------------------------------------------------------\n",
    "\n",
    "extract_features = 1,249,512\n",
    "tensor([[[ 0.1235,  1.0637,  0.3155,  ...,  0.1192, -0.0555,  0.3207],\n",
    "         [-0.0291, -0.0841, -0.1173,  ..., -0.0075,  0.0098, -0.0437],\n",
    "         [ 0.1854, -0.1252,  0.4169,  ...,  0.0302, -0.1354,  0.0635],\n",
    "         ...,\n",
    "         [-0.0499, -0.1683, -0.0749,  ..., -0.0380,  0.0608,  0.0732],\n",
    "         [ 0.1800,  0.1400,  2.0366,  ...,  0.0052,  0.1235,  0.0312],\n",
    "         [ 0.0764,  0.1193,  0.6772,  ...,  0.0497,  0.1072,  0.0665]]],\n",
    "       device='cuda:0', grad_fn=<GeluBackward0>)\n",
    "------------------------------------------------------------------\n",
    "hidden_states = 1,249,768\n",
    "tensor([[[ 0.0339, -0.4830, -0.0305,  ...,  0.0159,  0.0779, -0.2026],\n",
    "         [ 0.2883, -0.7021,  0.6997,  ...,  0.5885,  0.3514,  0.1212],\n",
    "         [-0.2194, -0.4437,  0.2657,  ..., -0.2675, -0.3728, -0.4760],\n",
    "         ...,\n",
    "         [-0.4071, -0.5601, -0.1708,  ..., -0.6891, -0.8761,  0.1655],\n",
    "         [-0.2948, -0.3244,  0.1004,  ...,  0.0792,  0.1473, -0.1143],\n",
    "         [-0.0085,  0.3465,  0.3159,  ...,  0.7847, -0.2695,  0.7689]]],\n",
    "       device='cuda:0', grad_fn=<ViewBackward0>) \n",
    "------------------------------------------------------------------\n",
    "extract_features = 1,249,512\n",
    "tensor([[[-0.9377, -0.0026,  0.2004,  ...,  1.1388, -1.0242,  0.4582],\n",
    "         [-0.8192, -0.6528, -0.4071,  ..., -0.6667,  0.5033, -0.5041],\n",
    "         [-0.3353,  0.6557,  1.2236,  ..., -0.5683,  0.2599,  0.5061],\n",
    "         ...,\n",
    "         [ 0.2780, -0.0862,  0.7484,  ..., -0.8934,  1.6097, -0.2075],\n",
    "         [-1.7482,  0.5112,  0.4290,  ...,  1.2316, -0.8526, -0.3602],\n",
    "         [-0.2306,  0.0931,  0.1837,  ...,  0.0942, -0.6955, -1.0463]]],\n",
    "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>) \n",
    "------------------------------------------------------------------\n",
    "\n",
    "hidden_states = 1,249, 768\n",
    "tensor([[[-0.1991,  0.3561,  0.1502,  ...,  0.0869,  0.6993,  0.1601],\n",
    "         [ 0.9603,  0.2272, -0.1970,  ...,  0.2197,  0.0703, -0.3534],\n",
    "         [-0.0289,  0.0383, -0.1532,  ...,  0.4002,  0.6341, -0.0135],\n",
    "         ...,\n",
    "         [ 0.0797, -0.0814,  0.6674,  ...,  0.1392,  0.4300, -0.2692],\n",
    "         [-0.3952, -0.6119, -0.5780,  ...,  0.7492,  0.2148,  0.2660],\n",
    "         [ 0.7736, -0.1803, -0.1786,  ..., -0.3715, -0.1962, -0.0951]]],\n",
    "------------------------------------------------------------------\n",
    "\n",
    "encoder_outputs (last hidden state) = [1, 249, 768])\n",
    "tensor([[[ 1.8529,  1.2018,  1.2603,  ..., -0.4005, -0.7679,  1.5168],\n",
    "         [ 2.4584,  0.3279,  1.1077,  ...,  0.0198,  0.5620,  1.3509],\n",
    "         [ 0.8397,  1.1146,  1.2582,  ...,  0.7337,  0.1549,  0.1386],\n",
    "         ...,\n",
    "         [ 2.3995,  0.0749,  0.3688,  ...,  0.1632, -0.3292,  0.8795],\n",
    "         [ 2.0309,  1.2241,  2.5985,  ..., -0.2022,  1.0152,  0.7052],\n",
    "         [-0.0648,  0.1555,  0.9045,  ...,  0.6808,  1.3329, -1.1029]]],\n",
    "------------------------------------------------------------------ <- pretraining model\n",
    "\n",
    "transformer_features = 1, 249, 256\n",
    "tensor([[[ 0.4395,  0.0093,  0.1106,  ...,  0.1624, -0.4259, -0.1438],\n",
    "         [ 0.2835,  0.0836, -0.1176,  ..., -1.4001, -0.6082, -0.4137],\n",
    "         [ 0.1038, -0.6754,  0.0953,  ...,  0.0870, -0.5074, -0.2045],\n",
    "         ...,\n",
    "         [ 0.7046, -0.7818, -0.0214,  ...,  0.1316, -0.0481, -0.3063],\n",
    "         [-0.4119,  0.0397,  0.2056,  ...,  0.2657,  0.0776,  0.1673],\n",
    "         [ 0.5931, -0.5297,  0.2475,  ..., -1.0861, -0.2295,  1.0191]]],\n",
    "       device='cuda:0', grad_fn=<ViewBackward0>) \n",
    "------------------------------------------------------------------\n",
    "\n",
    "extract_features = 1, 249, 512\n",
    "tensor([[[-0.5587, -1.2472, -1.0980,  ...,  0.8418, -0.5749,  0.4490],\n",
    "         [-0.7627, -0.4484, -0.7772,  ..., -0.2958,  1.2044, -0.6321],\n",
    "         [-0.8375, -0.0000, -0.9049,  ..., -0.9059,  0.1687, -0.6254],\n",
    "         ...,\n",
    "         [-0.0000, -0.0000, -2.3028,  ..., -0.5176,  3.0487, -0.0000],\n",
    "         [-1.0692, -0.0000, -0.8404,  ..., -0.3041,  1.6150, -0.0000],\n",
    "         [-0.0000,  0.0000, -0.4952,  ..., -0.0000,  2.0064,  0.7426]]],\n",
    "------------------------------------------------------------------\n",
    "\n",
    "quantized_features 1,249,256\n",
    "tensor([[[-0.0007,  0.0041,  0.0038,  ..., -0.0010,  0.0016,  0.0003],\n",
    "         [ 0.0035,  0.0055,  0.0023,  ...,  0.0107, -0.0053, -0.0056],\n",
    "         [-0.0015,  0.0167, -0.0020,  ..., -0.0018, -0.0030, -0.0007],\n",
    "         ...,\n",
    "         [ 0.0006, -0.0015, -0.0092,  ..., -0.0038,  0.0029,  0.0095],\n",
    "         [ 0.0033,  0.0141,  0.0015,  ..., -0.0038, -0.0048, -0.0006],\n",
    "         [ 0.0003, -0.0084, -0.0059,  ..., -0.0030, -0.0111, -0.0071]]]\n",
    "\n",
    "------------------------------------------------------------------<- into quantitation module\n",
    "\n",
    "hidden_states\n",
    "tensor([[[ 0.6000, -0.6655,  0.4842,  ..., -0.3797,  0.4714, -1.1692],\n",
    "         [ 0.2132, -0.8672,  0.2732,  ...,  0.2052,  0.5152,  0.3293],\n",
    "         [ 0.2438, -0.2918,  0.5199,  ..., -0.3120,  0.3674, -0.1998],\n",
    "         ...,\n",
    "         [ 0.7111,  0.0052, -0.3853,  ...,  0.0501, -0.6594,  0.5257],\n",
    "         [ 1.0093,  0.3563,  0.7620,  ..., -0.9669,  0.4358, -0.4640],\n",
    "         [ 0.8773, -0.3935, -0.9076,  ..., -0.9158,  0.4923,  0.1731]]],\n",
    "       device='cuda:0', grad_fn=<ViewBackward0>) torch.Size([1, 249, 640]) \n",
    "      \n",
    "tensor([[ 0.6000, -0.6655,  0.4842,  ...,  0.2033, -0.2749,  1.0719],<- then reshaped to this\n",
    "        [ 0.3865, -0.2972,  0.2173,  ..., -0.3797,  0.4714, -1.1692],\n",
    "        [ 0.2132, -0.8672,  0.2732,  ...,  0.4031,  0.8485,  0.5515],\n",
    "        ...,\n",
    "        [-0.1745,  0.4993, -0.1358,  ..., -0.9669,  0.4358, -0.4640],\n",
    "        [ 0.8773, -0.3935, -0.9076,  ...,  0.2689,  0.0802, -0.2527],\n",
    "        [ 0.1804,  0.3172, -0.1329,  ..., -0.9158,  0.4923,  0.1731]],\n",
    "       device='cuda:0', grad_fn=<ViewBackward0>) torch.Size([498, 320])\n",
    "\n",
    "------------------------------------------------------------------<- \n",
    "codevector_probs 498, 320\n",
    "\n",
    "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
    "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "        ...,\n",
    "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "        [0., 0., 0.,  ..., 0., 0., 0.]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
