{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 80000])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "import os\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "from IPython.display import Audio\n",
    "import io\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from config import Wav2Vec2Config\n",
    "from model import Wav2Vec2ForPreTraining,Wav2Vec2FeatureEncoder,Wav2Vec2GumbelVectorQuantizer,_compute_mask_indices,Wav2Vec2Encoder,Wav2Vec2FeatureProjection\n",
    "\n",
    "\n",
    "def resample_audio_torchaudio(file_path, original_sample_rate=44100, target_sample_rate=16000):\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "    if sample_rate != original_sample_rate:\n",
    "        raise ValueError(f\"Expected sample rate to be {original_sample_rate}, but got {sample_rate}\")\n",
    "    \n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=original_sample_rate, new_freq=target_sample_rate)\n",
    "    waveform = resampler(waveform)\n",
    "    \n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    \n",
    "    return waveform.squeeze(), target_sample_rate\n",
    "\n",
    "def load_audio(path):\n",
    "    waveform,sample_rate = torchaudio.load(path)\n",
    "\n",
    "    return waveform.mean(dim=0), sample_rate\n",
    "\n",
    "def load_dataset(file_list):\n",
    "    dataset = []\n",
    "    for file_path in file_list:\n",
    "        if file_path.endswith('.mp3'):\n",
    "            audio, sample_rate = resample_audio_torchaudio(file_path)\n",
    "            dataset.append(audio)\n",
    "    return torch.stack(dataset)\n",
    "\n",
    "\n",
    "dataset = load_dataset([f'data/mp3_train_files/Gould/Gould - WTC_clip_{i}.mp3' for i in range(1,4)])\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X -> Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent_reps.shape=torch.Size([3, 512, 249])\n"
     ]
    }
   ],
   "source": [
    "config = Wav2Vec2Config()\n",
    "\n",
    "\n",
    "feature_encoder = Wav2Vec2FeatureEncoder(config)\n",
    "# print(feature_encoder)\n",
    "\n",
    "\n",
    "latent_reps= feature_encoder(dataset)\n",
    "print(f\"{latent_reps.shape=}\") # batch,num_channels,cov_output_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "project Z to correct dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wav2Vec2FeatureProjection(\n",
      "  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (projection): Linear(in_features=512, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "hidden_states.shape=torch.Size([3, 249, 768])\n"
     ]
    }
   ],
   "source": [
    "feature_projection = Wav2Vec2FeatureProjection(config)\n",
    "print(feature_projection)\n",
    "\n",
    "hidden_states, extract_features = feature_projection(latent_reps.transpose(1,2))\n",
    "\n",
    "print(f\"{hidden_states.shape=}\")\n",
    "\n",
    "\n",
    "# then mask here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Wav2Vec2Encoder(config)\n",
    "\n",
    "encoder_outputs = encoder(hidden_states)\n",
    "\n",
    "hidden_states = encoder_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hidden_states,extract_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_hid = nn.Linear(config.hidden_size, config.proj_codevector_dim)\n",
    "dropout_features = nn.Dropout(config.feat_quantizer_dropout)\n",
    "\n",
    "transformer_features = project_hid(hidden_states)\n",
    "\n",
    "extract_features = dropout_features(extract_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 249, 512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 249, 256])\n"
     ]
    }
   ],
   "source": [
    "quantizer = Wav2Vec2GumbelVectorQuantizer(config)\n",
    "project_q = nn.Linear(config.codevector_dim, config.proj_codevector_dim)\n",
    "\n",
    "mask_time_indices = torch.tensor(_compute_mask_indices(shape=(extract_features.shape[0], extract_features.shape[1]), mask_prob=0.2, mask_length=2))\n",
    "\n",
    "quantized_features, codevector_perplexity = quantizer(extract_features,mask_time_indices=mask_time_indices) \n",
    "\n",
    "quantized_features = project_q(quantized_features)\n",
    "\n",
    "print(quantized_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Wav2Vec2Config()\n",
    "model = Wav2Vec2ForPreTraining(config)\n",
    "\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import _compute_mask_indices, _sample_negative_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_features = model.wav2vec2.feature_extractor(dataset).transpose(1, 2)\n",
    "batch_size, seq_len, _ = extract_features.shape\n",
    " \n",
    "attention_mask = torch.ones((batch_size, seq_len), dtype=torch.long) # no padding tokens\n",
    "\n",
    "\n",
    "mask_time_indices = _compute_mask_indices(\n",
    "        shape=(batch_size, seq_len),\n",
    "        mask_prob=config.mask_time_prob,\n",
    "        mask_length=config.mask_time_length,\n",
    "        attention_mask=attention_mask,\n",
    "        min_masks=config.mask_time_min_masks\n",
    "    )\n",
    "\n",
    "sampled_negative_indices = _sample_negative_indices(\n",
    "    features_shape=(batch_size, seq_len),\n",
    "    num_negatives=model.config.num_negatives,\n",
    "    mask_time_indices=mask_time_indices,\n",
    ")\n",
    "\n",
    "mask_time_indices = torch.tensor(mask_time_indices,dtype=torch.bool)\n",
    "\n",
    "sampled_negative_indices = torch.tensor(sampled_negative_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_features = model.wav2vec2.feature_extractor(dataset).transpose(1, 2)\n",
    "batch_size, seq_len, _ = extract_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 249, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_values = dataset #  Float values of input raw speech waveform.\n",
    "attention_mask = attention_mask  # bool tensor (batch_size, seq_len)\n",
    "mask_time_indices = mask_time_indices# bool tensor (batch_size, seq_len)\n",
    "sampled_negative_indices = sampled_negative_indices # bool tensor (batch_size, sequence_length, num_negatives)\n",
    "output_attentions = True\n",
    "output_hidden_states = False\n",
    "return_dict = torch.BoolTensor(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 80000])\n",
      "torch.Size([3, 249])\n",
      "torch.Size([3, 249])\n",
      "torch.Size([3, 249, 100])\n"
     ]
    }
   ],
   "source": [
    "print(input_values.shape)\n",
    "print(attention_mask.shape)\n",
    "print(mask_time_indices.shape)\n",
    "print(sampled_negative_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input_values=input_values,\n",
    "      attention_mask=attention_mask,\n",
    "      mask_time_indices=mask_time_indices,\n",
    "      sampled_negative_indices=sampled_negative_indices,\n",
    "      output_attentions=output_attentions,\n",
    "      output_hidden_states=output_hidden_states,\n",
    "      return_dict=return_dict)\n",
    "\n",
    "loss = out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(289.6899, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
