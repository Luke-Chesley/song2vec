{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 80000])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "import os\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "from IPython.display import Audio\n",
    "import io\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from config import Wav2Vec2Config\n",
    "from model import Wav2Vec2ForPreTraining,Wav2Vec2FeatureEncoder,Wav2Vec2GumbelVectorQuantizer,_compute_mask_indices,Wav2Vec2Encoder,Wav2Vec2FeatureProjection\n",
    "\n",
    "\n",
    "def resample_audio_torchaudio(file_path, original_sample_rate=44100, target_sample_rate=16000):\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "    if sample_rate != original_sample_rate:\n",
    "        raise ValueError(f\"Expected sample rate to be {original_sample_rate}, but got {sample_rate}\")\n",
    "    \n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=original_sample_rate, new_freq=target_sample_rate)\n",
    "    waveform = resampler(waveform)\n",
    "    \n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    \n",
    "    return waveform.squeeze(), target_sample_rate\n",
    "\n",
    "def load_audio(path):\n",
    "    waveform,sample_rate = torchaudio.load(path)\n",
    "\n",
    "    return waveform.mean(dim=0), sample_rate\n",
    "\n",
    "def load_dataset(file_list):\n",
    "    dataset = []\n",
    "    for file_path in file_list:\n",
    "        if file_path.endswith('.mp3'):\n",
    "            audio, sample_rate = resample_audio_torchaudio(file_path)\n",
    "            dataset.append(audio)\n",
    "    return torch.stack(dataset)\n",
    "\n",
    "\n",
    "dataset = load_dataset([f'data/mp3_train_files/Gould/Gould - WTC_clip_{i}.mp3' for i in range(1,4)])\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X -> Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent_reps.shape=torch.Size([3, 512, 249])\n"
     ]
    }
   ],
   "source": [
    "config = Wav2Vec2Config()\n",
    "\n",
    "\n",
    "feature_encoder = Wav2Vec2FeatureEncoder(config)\n",
    "# print(feature_encoder)\n",
    "\n",
    "\n",
    "latent_reps= feature_encoder(dataset)\n",
    "print(f\"{latent_reps.shape=}\") # batch,num_channels,cov_output_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "project Z to correct dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wav2Vec2FeatureProjection(\n",
      "  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (projection): Linear(in_features=512, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "hidden_states.shape=torch.Size([3, 249, 768])\n"
     ]
    }
   ],
   "source": [
    "feature_projection = Wav2Vec2FeatureProjection(config)\n",
    "print(feature_projection)\n",
    "\n",
    "hidden_states, extract_features = feature_projection(latent_reps.transpose(1,2))\n",
    "\n",
    "print(f\"{hidden_states.shape=}\")\n",
    "\n",
    "\n",
    "# then mask here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Wav2Vec2Encoder(config)\n",
    "\n",
    "encoder_outputs = encoder(hidden_states)\n",
    "\n",
    "hidden_states = encoder_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.9127, -0.6875, -1.4736,  ..., -2.1643,  0.8411, -0.0380],\n",
       "         [-1.4061, -1.0729, -1.2450,  ..., -1.4550, -0.0604,  0.0437],\n",
       "         [-1.2815, -0.5890, -1.6554,  ..., -1.7457,  0.7717, -0.3420],\n",
       "         ...,\n",
       "         [-1.1253,  0.2196, -1.4514,  ..., -1.4712,  1.0693, -0.1597],\n",
       "         [-0.9389,  0.5538, -0.6476,  ..., -1.0568,  0.7462, -1.1023],\n",
       "         [-1.1520,  0.5902, -0.0117,  ..., -0.3745,  1.1961, -0.8357]],\n",
       "\n",
       "        [[ 0.1138, -0.5742, -1.3258,  ..., -1.4461,  0.8528, -0.3117],\n",
       "         [-0.4957, -1.0862, -0.8850,  ..., -2.2565,  0.1077,  0.7734],\n",
       "         [-0.7777, -0.1501,  0.3099,  ..., -1.7332,  0.9596, -0.0120],\n",
       "         ...,\n",
       "         [-0.2798, -0.0862, -1.3628,  ..., -1.7271,  0.7087, -0.3084],\n",
       "         [ 0.5742,  0.3760,  0.4267,  ..., -1.7190,  1.3869, -0.7468],\n",
       "         [-2.1264,  1.0481,  0.1977,  ..., -0.9605,  0.5391,  0.3180]],\n",
       "\n",
       "        [[-0.9347,  0.1893, -0.9451,  ..., -0.4437, -0.2000, -0.5965],\n",
       "         [-1.6724, -0.2969, -1.0850,  ..., -0.2700, -0.6010, -0.0190],\n",
       "         [-0.0953, -0.6762, -1.0353,  ..., -1.5395, -0.2381,  0.0698],\n",
       "         ...,\n",
       "         [-0.3467, -0.2247, -0.9488,  ..., -1.5113, -0.2810, -1.1948],\n",
       "         [-0.9657,  0.1968, -0.7672,  ..., -2.2485,  1.0411, -0.1227],\n",
       "         [ 0.0893, -0.5490, -0.7787,  ..., -1.3961, -0.3730, -0.1813]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states,extract_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_hid = nn.Linear(config.hidden_size, config.proj_codevector_dim)\n",
    "dropout_features = nn.Dropout(config.feat_quantizer_dropout)\n",
    "\n",
    "transformer_features = project_hid(hidden_states)\n",
    "\n",
    "extract_features = dropout_features(extract_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 249, 512])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 249, 256])\n"
     ]
    }
   ],
   "source": [
    "quantizer = Wav2Vec2GumbelVectorQuantizer(config)\n",
    "project_q = nn.Linear(config.codevector_dim, config.proj_codevector_dim)\n",
    "\n",
    "mask_time_indices = torch.tensor(_compute_mask_indices(shape=(extract_features.shape[0], extract_features.shape[1]), mask_prob=0.2, mask_length=2))\n",
    "\n",
    "quantized_features, codevector_perplexity = quantizer(extract_features,mask_time_indices=mask_time_indices) \n",
    "\n",
    "quantized_features = project_q(quantized_features)\n",
    "\n",
    "print(quantized_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Wav2Vec2Config()\n",
    "model = Wav2Vec2ForPreTraining(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ..., -0.0183, -0.0127, -0.0075]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_values = [] #  Float values of input raw speech waveform.\n",
    "attention_mask = []  # bool tensor (batch_size, seq_len)\n",
    "mask_time_indices = []# bool tensor (batch_size, seq_len)\n",
    "sampled_negative_indices = [] # bool tensor (batch_size, sequence_length, num_negatives)\n",
    "output_attentions = torch.BoolTensor(0)\n",
    "output_hidden_states = torch.BoolTensor(0)\n",
    "return_dict = torch.BoolTensor(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForPreTrainingOutput(loss=None, projected_states=tensor([[[ 0.3807, -0.4252,  0.3157,  ..., -0.1948,  0.0573,  1.0749],\n",
       "         [-0.6598, -0.2952, -0.6157,  ..., -0.2388,  0.6263,  0.7527],\n",
       "         [-0.4502, -0.5760,  0.4205,  ..., -0.6499,  0.4563,  0.8842],\n",
       "         ...,\n",
       "         [-0.4406,  0.0493, -0.0269,  ...,  0.1158,  0.1032,  0.3635],\n",
       "         [ 0.2953, -0.0126,  0.1722,  ..., -0.0835,  0.6201,  0.1347],\n",
       "         [-0.5455, -0.4256, -0.0034,  ..., -0.0527, -0.3368,  0.4376]]],\n",
       "       grad_fn=<ViewBackward0>), projected_quantized_states=tensor([[[-0.0051,  0.0159,  0.0079,  ...,  0.0671,  0.0092,  0.0474],\n",
       "         [-0.0108, -0.0311,  0.0222,  ...,  0.0232, -0.0338, -0.0647],\n",
       "         [ 0.0004,  0.0127,  0.0092,  ..., -0.0064, -0.0087,  0.0164],\n",
       "         ...,\n",
       "         [ 0.0073, -0.0303, -0.0054,  ..., -0.0123,  0.0036, -0.0212],\n",
       "         [-0.0235,  0.0589,  0.0228,  ...,  0.0305, -0.0358, -0.0126],\n",
       "         [ 0.0592, -0.0347,  0.0043,  ...,  0.0401,  0.0288, -0.0091]]],\n",
       "       grad_fn=<ViewBackward0>), codevector_perplexity=tensor(635.8691, grad_fn=<SumBackward0>), hidden_states=None, attentions=None, contrastive_loss=None, diversity_loss=None)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(dataset[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
